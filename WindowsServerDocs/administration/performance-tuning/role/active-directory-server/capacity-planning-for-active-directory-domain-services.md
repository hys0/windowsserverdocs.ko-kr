---
title: Active Directory Domain Services에 대 한 용량 계획
description: AD DS에 대 한 용량 계획 중에 고려해 야 할 요소에 대 한 자세한 설명입니다.
ms.prod: windows-server
ms.technology: performance-tuning-guide
ms.topic: article
ms.author: v-tea; kenbrunf
author: teresa-motiv
ms.date: 7/3/2019
ms.openlocfilehash: fc3f1dce4bb88d8581e3d8a890e3c121badaba71
ms.sourcegitcommit: 6d7a394edefba684f7b6983c65026679c1b7a485
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 06/15/2020
ms.locfileid: "84776725"
---
# <a name="capacity-planning-for-active-directory-domain-services"></a>Active Directory Domain Services에 대 한 용량 계획

이 항목은 원래 켄은 Brumfield, Microsoft의 선임 프리미어 현장 엔지니어에 의해 작성 되었으며 Active Directory Domain Services (AD DS)에 대 한 용량 계획에 대 한 권장 사항을 제공 합니다.

## <a name="goals-of-capacity-planning"></a>용량 계획의 목표

용량 계획은 성능 문제를 해결 하는 것과는 다릅니다. 이러한 항목은 밀접 하 게 관련 되어 있지만 매우 다릅니다. 용량 계획의 목표는 다음과 같습니다.

- 환경 올바르게 구현 및 운영
- 성능 문제를 해결 하는 데 소요 된 시간을 최소화 합니다.

용량 계획에서 조직은 클라이언트 성능 요구 사항을 충족 하 고 데이터 센터의 하드웨어를 업그레이드 하는 데 필요한 시간을 수용할 수 있도록 피크 기간 동안 40% 프로세서 사용률의 기준 목표를 가질 수 있습니다. 하지만 비정상 성능 인시던트에 대 한 알림을 받으려면 모니터링 경고 임계값을 5 분 간격으로 90%로 설정할 수 있습니다.

차이점은 용량 관리 임계값을 지속적으로 초과 하는 경우 (일회성 이벤트는 문제가 되지 않음) 용량 추가 (즉, 더 많은 또는 더 빠른 프로세서에서 추가)는 솔루션이 되거나 여러 서버에서 서비스를 확장 하는 것입니다. 성능 경고 임계값은 클라이언트 환경이 현재 발생 하 고 있으며 문제를 해결 하기 위해 즉각적인 단계가 필요 함을 의미 합니다.

이에 대 한 비유로 용량 관리는 자동차 사고를 방지 하 고 (방어 구동, brakes가 제대로 작동 하는지 확인 하는 등), 성능 문제 해결은 사고 후에 수행 하는 경찰, 화재 부서 및 비상 의료 전문가에 게 있습니다. 이는 "방어 주행" Active Directory 스타일입니다.

최근 몇 년 동안 강화 시스템에 대 한 용량 계획 지침이 대폭 변경 되었습니다. 시스템 아키텍처에서 다음과 같이 변경 하면 서비스를 디자인 하 고 크기를 조정 하는 것에 대 한 근본적인 가정이 있습니다.

- 64 비트 서버 플랫폼
- 가상화
- 전력 소비에 대 한 주의가 증가 함
- SSD 저장소
- 클라우드 시나리오

또한이 접근 방식은 서버 기반 용량 계획 연습에서 서비스 기반 용량 계획 연습으로 전환 됩니다. 많은 Microsoft 및 타사 제품이 백 엔드로 사용 하는 성숙한 분산 서비스인 Active Directory Domain Services (AD DS)는 다른 응용 프로그램을 실행 하는 데 필요한 용량을 보장 하기 위해 올바르게 계획 하는 데 가장 중요 한 제품이 됩니다.

### <a name="baseline-requirements-for-capacity-planning-guidance"></a>용량 계획 지침에 대 한 기준 요구 사항

이 문서 전체에서 다음과 같은 기본 요구 사항이 필요 합니다.

- 독자가 [Windows Server 2012 r 2에 대 한 성능 조정 지침](https://docs.microsoft.com/previous-versions//dn529133(v=vs.85))을 읽고 익숙합니다.
- Windows Server 플랫폼은 x64 기반 아키텍처입니다. 그러나 Active Directory 환경이 Windows Server 2003 x 86 (현재 지원 수명 주기 종료 이후)에 설치 되어 있는 경우에도, 크기가 1.5 GB 미만이 고 메모리에 쉽게 보관할 수 있는 DIT (디렉터리 정보 트리)가 있는 경우에도이 문서의 지침은 적용 됩니다.
- 용량 계획은 연속 프로세스로, 환경에 얼마나 잘 부합 하는지 정기적으로 검토 해야 합니다.
- 하드웨어 비용이 변경 됨에 따라 여러 하드웨어 수명 주기 동안 최적화가 발생 합니다. 예를 들어, 메모리가 더 저렴 하거나, 코어 당 비용이 감소 하거나, 다양 한 저장소 옵션의 가격이 변경 됩니다.
- 하루 중 사용량이 많은 기간을 계획 합니다. 30 분 또는 시간 간격으로이를 확인 하는 것이 좋습니다. 모든 항목은 실제 최고 수를 숨기고 "일시적인 급증"으로 인해 더 이상 왜곡 될 수 있습니다.
- 기업의 하드웨어 수명 주기 과정에서 성장을 계획 합니다. 여기에는 지연 된 방식으로 하드웨어를 업그레이드 하거나 추가 하는 전략 또는 3 ~ 5 년 마다 전체 새로 고침이 포함 될 수 있습니다. 각에는 Active Directory 부하가 증가 하는 만큼의 "추측"이 필요 합니다. 기록 데이터 (수집 된 경우)는이 평가에 도움이 됩니다.
- 내결함성을 계획 합니다. 예상치 *n* 이 파생 되 면 *n* &ndash; 1, *n* &ndash; 2, *n* &ndash; *x*를 포함 하는 시나리오를 계획 합니다.
  - 조직 요구 사항에 따라 추가 서버에를 추가 하 여 단일 또는 여러 서버 손실이 최대 용량 예상치를 초과 하지 않도록 해야 합니다.
  - 또한 증가 계획 및 내결함성 계획을 통합 해야 한다고 가정 합니다. 예를 들어 한 DC가 로드를 지 원하는 데 필요 하지만 다음 연도에 부하가 두 배가 되 고 총 두 개의 Dc가 필요한 경우 내결함성을 지원 하기에 충분 한 용량이 없게 됩니다. 이 솔루션은 세 개의 Dc로 시작 해야 합니다. 예산에 근접 한 경우 3 또는 6 개월 후에 세 번째 DC를 추가 하도록 계획할 수도 있습니다.

    > [!NOTE]
    > 응용 프로그램 서버 또는 클라이언트에서 로드가 발생 하는지 여부에 상관 없이 Active Directory 인식 응용 프로그램을 추가 하면 DC 로드에 큰 영향을 미칠 수 있습니다.

### <a name="three-step-process-for-the-capacity-planning-cycle"></a>용량 계획 주기에 대 한 3 단계 프로세스

용량 계획에서 먼저 필요한 서비스 품질을 결정 합니다. 예를 들어 핵심 데이터 센터는 더 높은 수준의 동시성을 지원 하 고 사용자가 응용 프로그램을 사용 하는 데 더 일관 된 환경을 필요로 하므로 중복성 및 시스템 및 인프라 병목 상태를 최소화 하는 데 더 많은 주의가 필요 합니다. 이와 대조적으로 소수의 사용자가 있는 위성 위치에는 동일한 수준의 동시성 또는 내결함성을 요구 하지 않습니다. 따라서 위성 사무실은 기본 하드웨어 및 인프라를 최적화 하는 데 많은 주의가 필요 하지 않을 수 있으며,이로 인해 비용을 절감할 수 있습니다. 여기에 제공 되는 모든 권장 사항과 지침은 최적의 성능을 위한 것 이며 요구 사항이 낮은 시나리오의 경우 선택적으로 완화 될 수 있습니다.

다음 질문은 가상화 됨 또는 실제 입니까? 용량 계획 관점에서 올바른 답변이 나 잘못 된 대답은 없습니다. 사용할 변수 집합은 서로 다릅니다. 가상화 시나리오는 다음 두 가지 옵션 중 하나로 제공 됩니다.

- 호스트 당 하나의 게스트를 사용 하는 "직접 매핑" (가상화는 서버에서 물리적 하드웨어를 추상화 하는 용도로만 존재 함)
- "공유 호스트"

테스트 및 프로덕션 시나리오는 "직접 매핑" 시나리오를 실제 호스트와 동일 하 게 처리할 수 있음을 의미 합니다. 그러나 "공유 호스트"는 나중에 자세히 설명 하는 다양 한 고려 사항을 소개 합니다. "공유 호스트" 시나리오는 AD DS 리소스에 대해 경쟁 하는 것을 의미 하며,이를 위해 페널티 및 튜닝 고려 사항이 있습니다.

이러한 고려 사항을 염두에 두면 용량 계획 주기는 반복적인 3 단계 프로세스입니다.

1. 기존 환경을 측정 하 고, 시스템 병목 현상의 현재 위치를 확인 하 고, 필요한 용량을 계획 하는 데 필요한 환경 기본 사항을 확인 하세요.
1. 1 단계에 설명 된 조건에 따라 필요한 하드웨어를 확인 합니다.
1. 구현 된 인프라가 사양 내에서 작동 하는지 모니터링 하 고 유효성을 검사 합니다. 이 단계에서 수집 되는 데이터 중 일부는 용량 계획의 다음 주기에 대 한 기준이 됩니다.

### <a name="applying-the-process"></a>프로세스 적용

성능을 최적화 하려면 이러한 주요 구성 요소를 올바르게 선택 하 고 응용 프로그램 로드에 맞게 조정 해야 합니다.

1. 메모리
1. 네트워크
1. 스토리지
1. 프로세서
1. Net Logon

AD DS의 기본 저장소 요구 사항 및 잘 작성 된 클라이언트 1만 소프트웨어의 일반적인 동작을 사용 하면 대부분의 최신 서버 클래스 시스템이 부하를 처리 하기 때문에 2만 사용자가 실제 하드웨어와 관련 된 용량 계획에 많은 투자를 하지 않게 수 있습니다. 즉, 다음 표에는 올바른 하드웨어를 선택 하기 위해 기존 환경을 평가 하는 방법이 요약 되어 있습니다. 다음 섹션에서는 각 구성 요소를 자세히 분석 하 여 관리자가 기준 권장 사항 및 환경 특정 보안 주체를 사용 하 여 인프라를 평가 AD DS 수 있도록 합니다.

일반적으로:

- 현재 데이터를 기반으로 하는 모든 크기 조정은 현재 환경에 대해서만 정확 합니다.
- 예상치는 하드웨어 수명 주기 동안 수요를 증가 시킬 것으로 예상 합니다.
- 현재를 과도 하 게 증가 시키고 더 큰 환경으로 증가 하는지 확인 하거나 수명 주기 동안 용량을 추가 합니다.
- 가상화를 위해 가상화의 오버 헤드를 관련 된 모든 도메인에 추가 해야 한다는 점을 제외 하 고 동일한 용량 계획 보안 주체 및 방법론을 모두 적용 합니다.
- 예측을 시도 하는 것과 같은 용량 계획은 정확한 과학이 아닙니다. 정확 하 게 계산 하는 것은 불가능 하며 100% 정확도로도 됩니다. 여기서 지침은 leanest 권장 사항입니다. 추가 안전을 위해 용량을 추가 하 고 대상에 환경이 유지 되는지 지속적으로 확인 합니다.

### <a name="data-collection-summary-tables"></a>데이터 수집 요약 테이블

#### <a name="new-environment"></a>새 환경

| 구성 요소 | 있다고 |
|-|-|
|저장소/데이터베이스 크기|각 사용자에 대해 40 KB ~ 60|
|RAM|데이터베이스 크기<br />기본 운영 체제 권장 사항<br />타사 애플리케이션|
|네트워크|1GB|
|CPU|1000 각 코어에 대 한 동시 사용자 수|

#### <a name="high-level-evaluation-criteria"></a>상위 수준 평가 기준

| 구성 요소 | 평가 조건 | 고려 사항 계획 |
|-|-|-|
|저장소/데이터베이스 크기|[저장소 제한](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/cc961769(v=technet.10)) 에서 "조각 모음이 해제 된 디스크 공간의 로깅을 활성화 하려면" 섹션| |
|저장소/데이터베이스 성능|<ul><li>"논리 디스크 ( *\<NTDS Database Drive\>* ) \Avg disk sec/Read," "논리 디스크 ( *\<NTDS Database Drive\>* ) \Avg disk Sec/Write," "논리 디스크 ( *\<NTDS Database Drive\>* ) \avg Disk sec/Transfer"</li><li>"논리 디스크 ( *\<NTDS Database Drive\>* ) \ Reads/sec," "논리 디스크 ( *\<NTDS Database Drive\>* ) \ Writes/sec," "논리 디스크 ( *\<NTDS Database Drive\>* ) \ 전송/초"</li></ul>|<ul><li>저장소에 두 가지 문제 해결<ul><li>사용 가능한 공간 (오늘날의 스핀 들 기반 크기 및 SSD 기반 저장소)은 대부분의 AD 환경에서 관련이 없습니다.</li> <li>입력/출력 (IO) 작업 사용 가능-대부분의 환경에서는 이러한 경우가 종종 간과 됩니다. 그러나 전체 NTDS 데이터베이스를 메모리로 로드 하는 데 충분 한 RAM이 없는 환경만 평가 하는 것이 중요 합니다.</li></ul><li>저장소는 복잡 한 토픽 일 수 있으며, 적절 한 크기 조정을 위해 하드웨어 공급 업체의 전문 지식을 포함 해야 합니다. 특히 SAN, NAS 및 iSCSI 시나리오와 같은 더 복잡 한 시나리오를 사용 합니다. 그러나 일반적으로 저장소 기가바이트 당 비용은 IO 당 비용을 직접 동적일 하는 경우가 많습니다.<ul><li>Raid 5는 Raid 1 보다 Gb 당 저렴 한 비용을 갖지만 Raid 1은 IO 당 비용이 저렴 합니다.</li><li>스핀 들 기반 하드 드라이브의 요금은 Gb 당 저렴 하지만 Ssd는 IO 당 비용이 저렴 합니다.</li></ul><li>컴퓨터 또는 Active Directory Domain Services 서비스를 다시 시작 하 고 나면 ESE (Extensible Storage Engine) 캐시가 비어 있고 캐시의 성능이 저하 되는 동안 성능이 디스크에 바인딩됩니다.</li><li>대부분의 환경에서 AD는 디스크에 대 한 임의 패턴에서 읽기 집약적 i/o를 사용 하 여 캐싱 및 읽기 최적화 전략의 이점을 크게 부정 합니다.  또한 AD는 대부분의 저장소 시스템 캐시 보다 메모리의 캐시 크기를 늘립니다.</li></ul>
|RAM|<ul><li>데이터베이스 크기</li><li>기본 운영 체제 권장 사항</li><li>타사 애플리케이션</li></ul>|<ul><li>저장소는 컴퓨터에서 가장 느린 구성 요소입니다. RAM에 상주할 수 있는 것이 많을 수록 디스크로 이동 해야 하는 것은 줄어듭니다.</li><li>운영 체제, 에이전트 (바이러스 백신, 백업, 모니터링), NTDS 데이터베이스 및 시간에 따른 증가를 저장 하는 데 충분 한 RAM이 할당 되었는지 확인 합니다.</li><li>RAM의 양을 최대화 (예: 위성 위치) 하거나 사용할 수 없는 환경 (DIT가 너무 큼)의 경우 저장소 섹션을 참조 하 여 저장소 크기를 적절히 조정 해야 합니다.</li></ul>|
|네트워크|<ul><li>"네트워크 인터페이스 ( \* ) \ 받은 바이트 수/초"</li><li>"네트워크 인터페이스 ( \* ) \ 보낸 바이트/초"|<ul><li>일반적으로 DC에서 전송 된 트래픽은 DC로 전송 되는 트래픽을 초과 합니다.</li><li>전환 된 이더넷 연결은 전이중 이므로 인바운드 및 아웃 바운드 네트워크 트래픽의 크기를 독립적으로 조정 해야 합니다.</li><li>Dc 수를 통합 하면 각 DC에 대 한 클라이언트 요청에 응답을 다시 보내는 데 사용 되는 대역폭의 양이 증가 하지만 사이트 전체에 대해 선형에는 충분히 가깝습니다.</li><li>위성 위치 Dc를 제거 하는 경우에는 위성 DC의 대역폭을 허브 Dc에 추가 하 고이를 사용 하 여 얼마나 많은 WAN 트래픽이 발생 하는지 평가 해야 합니다.</li></ul>|
|CPU|<ul><li>"논리 디스크 ( *\<NTDS Database Drive\>* ) \Avg disk sec/Read"</li><li>"Process (lsass) \\ % Processor Time"</li></ul>|<ul><li>병목 현상으로 저장소를 제거 하 고 나 서 필요한 계산 능력의 양을 해결 합니다.</li><li>완전 한 선형은 아니지만 특정 범위 (예: 사이트) 내의 모든 서버에서 사용 되는 프로세서 코어 수를 사용 하 여 총 클라이언트 부하를 지 원하는 데 필요한 프로세서 수를 측정할 수 있습니다. 범위 내의 모든 시스템에서 현재 서비스 수준을 유지 하는 데 필요한 최소를 추가 합니다.</li><li>전원 관리 관련 변경 내용을 포함 하 여 프로세서 속도를 변경 하면 현재 환경에서 파생 된 번호가 영향을 받습니다. 일반적으로 2.5 GHz 프로세서에서 3 GHz 프로세서로 전환 하는 방법을 정확 하 게 평가 하는 것은 필요한 Cpu 수를 줄이는 것이 불가능 합니다.</li></ul>|
|NetLogon|<ul><li>"Netlogon ( \* ) \Semaphore 획득"</li><li>"Netlogon ( \* ) \Semaphore 시간 제한"</li><li>"Netlogon ( \* ) \ 평균 세마포 보류 시간"</li></ul>|<ul><li>Net Logon 보안 채널/MaxConcurrentAPI NTLM 인증 및/또는 PAC 유효성 검사를 사용 하는 환경에만 영향을 줍니다. PAC 유효성 검사는 Windows Server 2008 이전 버전의 운영 체제에서 기본적으로 설정 되어 있습니다. 이는 클라이언트 설정 이므로 모든 클라이언트 시스템에서이를 끌 때까지 Dc에 영향을 줍니다.</li><li>포리스트 간 트러스트를 포함 하는 중요 한 상호 신뢰 인증을 사용 하는 환경에서는 크기가 적절 하지 않은 경우에는 위험이 높아집니다.</li><li>서버를 통합 하면 상호 신뢰 간 인증의 동시성이 증가 합니다.</li><li>사용자가 새 클러스터 노드에 집단를 다시 인증할 때 클러스터 failover)가 등의 서 수를 수용 해야 합니다.</li><li>개별 클라이언트 시스템 (예: 클러스터)에도 조정이 필요할 수 있습니다.</li></ul>|

## <a name="planning"></a>계획

오랜 시간 동안 AD DS 크기 조정에 대 한 커뮤니티 권장 사항은 "데이터베이스 크기로 RAM을 많이 배치" 하는 것입니다. 대부분의 경우이 권장 사항은 대부분의 환경에서 걱정 해야 하는 것입니다. 그러나 AD DS를 사용 하는 에코 시스템은 AD DS 환경 1999 자체를 사용 하는 것 보다 훨씬 더 큽니다. 계산 능력과 x 86 아키텍처에서 x64 아키텍처로 전환 하는 것은 실제 하드웨어에서 AD DS를 실행 하는 대규모 고객 집합과 관련이 없는 성능에 대 한 크기 조정의 측면을 subtler 때문에 가상화의 증가로 인해 이전 보다 더 많은 대상에 튜닝 문제가 다시 도입 되었습니다.

다음 지침은 실제, 가상/물리적 조합 또는 가상화 된 시나리오 중 어디에 배포 되는지에 관계 없이 서비스로 Active Directory 요구를 확인 하 고 계획 하는 방법에 대 한 것입니다. 따라서 저장소, 메모리, 네트워크 및 프로세서의 네 가지 주요 구성 요소 각각에 대 한 평가를 중단 합니다. 간단히 말해서 AD DS의 성능을 최대화 하기 위해 가능한 한 프로세서의 범위를 최대한 활용 하는 것이 목표입니다.

## <a name="ram"></a>RAM

RAM으로 캐시 될 수 있는 더 많은 경우 디스크로 이동 해야 하는 것은 줄어듭니다. 서버의 확장성을 최대화 하려면 최소 RAM 용량이 현재 데이터베이스 크기, 총 SYSVOL 크기, 운영 체제 권장 금액 및 에이전트에 대 한 공급 업체 권장 사항 (바이러스 백신, 모니터링, 백업 등)의 합계 여야 합니다. 서버의 수명 동안 증가를 수용할 수 있도록 추가 금액이 추가 되어야 합니다. 이는 환경적 변화에 따른 데이터베이스 증가의 예측을 기반으로 하는 environmentally 주관적입니다.

RAM의 양을 최대화 (예: 위성 위치) 하거나 사용할 수 없는 환경 (DIT가 너무 큼)의 경우 저장소 섹션을 참조 하 여 저장소가 제대로 설계 되었는지 확인 합니다.

메모리 크기 조정의 일반 컨텍스트에서 제공 되는 필연적인 결과로는 페이지 파일의 크기를 조정 합니다. 다른 모든 메모리와 동일한 컨텍스트에서는 훨씬 느린 디스크로의 연결을 최소화 하는 것이 목표입니다. 따라서 "페이지 파일의 크기를 조정 해야 하는 것은 무엇입니까?"로 이동 해야 합니다. "페이징을 최소화 하는 데 필요한 RAM은 얼마 인가요?" 후자 질문에 대 한 답변은이 섹션의 나머지 부분에 설명 되어 있습니다. 이렇게 하면 일반적인 운영 체제 권장 사항의 영역으로 페이지 파일의 크기를 조정 하 고, 메모리 덤프를 위해 시스템을 구성 해야 하는 대부분의 토론은 AD DS 성능과 관련이 없습니다.

### <a name="evaluating"></a>Evaluating

DC (도메인 컨트롤러)에서 필요로 하는 RAM의 양은 실제로 다음과 같은 이유 때문에 복잡 한 방법입니다.

- 기존 시스템을 사용 하 여 LSASS가 메모리 압력 조건에 따라 트리밍하는 요구를 인위적으로 deflating 하는 데 필요한 RAM의 양을 측정 하는 경우에 높은 오류가 발생할 수 있습니다.
- 개별 DC가 클라이언트에 "흥미로운" 항목을 캐시 하기만 하면 되는 주관적인 사실입니다. 즉, Exchange 서버만 사용 하는 사이트의 DC에 캐시 되어야 하는 데이터는 사용자를 인증 하는 DC에 캐시 해야 하는 데이터와 매우 다릅니다.
- 사례별로 각 DC의 RAM을 평가 하는 것은 매우 제한적 이며 환경이 변경 됨에 따라 변경 됩니다.
- 권장 사항 뒤의 조건은 의사 결정을 내리는 데 도움이 됩니다.
- RAM에 캐시할 수 있는 것이 많을 수록 디스크로 이동 해야 하는 것은 줄어듭니다.
- 저장소는 컴퓨터의 가장 느린 구성 요소입니다. 스핀 들 기반 및 SSD 저장소 미디어에 있는 데이터에 대 한 액세스는 RAM의 데이터에 액세스 하는 것 보다 1, 000, 000x의 순서로 느립니다.

따라서 서버의 확장성을 최대화 하기 위해 최소 RAM 양은 현재 데이터베이스 크기, 총 SYSVOL 크기, 운영 체제 권장 용량, 에이전트에 대 한 공급 업체 권장 사항 (바이러스 백신, 모니터링, 백업 등)의 합계입니다. 서버 수명 동안 증가를 수용 하기 위해 추가 금액을 추가 합니다. 이는 데이터베이스 증가의 예측에 따라 주관적인 environmentally 됩니다. 그러나 최종 사용자 집합이 작은 위성 위치의 경우 이러한 사이트는 대부분의 요청을 처리 하기 위해 캐시 하지 않아도 되므로 이러한 요구 사항을 완화할 수 있습니다.

RAM의 양을 최대화 (예: 위성 위치) 하거나 사용할 수 없는 환경 (DIT가 너무 큼)의 경우 저장소 섹션을 참조 하 여 저장소 크기를 적절히 조정 해야 합니다.

> [!NOTE]
> 메모리 크기를 조정 하는 동안 필연적인 결과로는 페이지 파일의 크기를 조정 합니다. 훨씬 느린 디스크로 이동 하는 것을 최소화 하는 것이 목표 이기 때문에 "페이지 파일의 크기를 조정 하는 방법" "페이징을 최소화 하는 데 필요한 RAM은 얼마 인가요?" 후자 질문에 대 한 답변은이 섹션의 나머지 부분에 설명 되어 있습니다. 이렇게 하면 일반적인 운영 체제 권장 사항의 영역으로 페이지 파일의 크기를 조정 하 고, 메모리 덤프를 위해 시스템을 구성 해야 하는 대부분의 토론은 AD DS 성능과 관련이 없습니다.

### <a name="virtualization-considerations-for-ram"></a>RAM에 대 한 가상화 고려 사항

호스트에서 메모리를 과도 하 게 커밋하는 것을 방지 합니다. RAM의 양을 최적화 하는 기본적인 목표는 디스크에 소요 되는 시간을 최소화 하는 것입니다. 가상화 시나리오에서 게스트에 더 많은 RAM이 할당 되는 메모리 초과 커밋의 개념은 물리적 컴퓨터에 존재 합니다. 그 자체로는 문제가 되지 않습니다. 모든 게스트가 적극적으로 사용 하는 총 메모리가 호스트의 RAM 크기를 초과 하 고 기본 호스트가 페이징을 시작할 때 문제가 됩니다. 도메인 컨트롤러에서 데이터를 가져오기 위해 ntds.dit로 이동 하거나, 도메인 컨트롤러가 데이터를 가져오기 위해 페이지 파일로 이동 하거나, 게스트에서 인식 하는 것으로 간주 되는 데이터를 가져오기 위해 호스트가 디스크로 이동 하는 경우에는 디스크가 바인딩됩니다.

### <a name="calculation-summary-example"></a>계산 요약 예

|구성 요소|예상 메모리 (예제)|
|-|-|
|기본 운영 체제 권장 RAM (Windows Server 2008)|2GB|
|LSASS 내부 작업|200MB|
|모니터링 에이전트|100MB|
|바이러스 백신|100MB|
|데이터베이스 (글로벌 카탈로그)|8.5 GB |
|백업을 실행 하기 위한 것입니다. 관리자는 영향 없이 로그온 합니다.|1GB|
|합계|12GB|

**권장: 16gb**

시간이 지남에 따라 데이터베이스에 더 많은 데이터를 추가 하 고 서버는 3 ~ 5 년 동안 프로덕션 환경에 있을 수 있습니다. 33%의 예상 증가에 따라 16gb는 물리적 서버에 배치 하는 데 적절 한 크기의 RAM이 됩니다. 가상 컴퓨터에서 설정을 수정 하 고, RAM을 VM에 추가할 수 있는 편의성을 제공 하 고, 나중에 모니터링 하 고 업그레이드 하는 계획을 사용 하는 것이 합리적입니다.

## <a name="network"></a>네트워크

### <a name="evaluating"></a>Evaluating

이 섹션은 WAN을 통과 하는 트래픽에 중점을 두는 복제 트래픽에 대 한 요구 사항을 평가 하는 방법에 대해 자세히 설명 합니다 .이는 클라이언트 쿼리, 그룹 정책 응용 프로그램 등을 포함 하 여 필요한 총 대역폭과 네트워크 용량을 평가 하는 것과 관련 하 여 [Active Directory 복제 트래픽에](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/bb742457(v=technet.10))대해 자세히 설명 합니다. 기존 환경의 경우 "Network Interface ( \* ) \ Bytes Received/sec" 및 "Network interface ( \* ) \Bytes Sent/sec" 성능 카운터를 사용 하 여이를 수집할 수 있습니다. 15, 30 또는 60 분의 네트워크 인터페이스 카운터에 대 한 샘플 간격입니다. 더 작은 것은 일반적으로 적절 한 측정을 위해 너무 휘발성입니다. 더 큰 것은 매일 피킹을 과도 하 게 완만 하 게 됩니다.

> [!NOTE]
> 일반적으로 dc의 네트워크 트래픽 대부분은 DC가 클라이언트 쿼리에 응답 하므로 아웃 바운드 됩니다. 아웃 바운드 트래픽에 중점을 두어야 하는 이유입니다. 하지만 인바운드 트래픽에 대해 각 환경을 평가 하는 것이 좋습니다. 동일한 방법을 사용 하 여 인바운드 네트워크 트래픽 요구 사항을 해결 하 고 검토할 수 있습니다. 자세한 내용은 기술 자료 문서 [929851: Windows Vista 및 Windows Server 2008에서 tcp/ip의 기본 동적 포트 범위가 변경 되었습니다](https://support.microsoft.com/kb/929851).를 참조 하세요.

### <a name="bandwidth-needs"></a>대역폭 요구 사항

네트워크 확장성 계획에는 트래픽 양과 네트워크 트래픽의 CPU 로드 라는 두 가지 범주가 포함 됩니다. 이러한 각 시나리오는이 문서의 다른 항목과 비교 하 여 바로 진행 됩니다.

지원 되어야 하는 트래픽 양을 평가할 때 네트워크 트래픽 측면에서 AD DS에 대 한 두 가지 고유 범주의 용량 계획을 세울 수 있습니다. 첫 번째는 도메인 컨트롤러 간에 트래버스하는 복제 트래픽 이며 [Active Directory 복제 트래픽에](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/bb742457(v=technet.10)) 대 한 철저 한 설명 이며 현재 AD DS 버전과 관련이 있습니다. 두 번째는 사이트 간 클라이언트-서버 트래픽입니다. 계획에 대 한 간단한 시나리오 중 하나는 사이트 간 트래픽은 클라이언트에 다시 전송 된 대량의 데이터를 기준으로 클라이언트의 작은 요청을 주로 수신 합니다. 일반적으로 사이트에서 서버당 최대 5000 명의 사용자에 게는 100 MB가 적합 합니다. 5000 명 이상의 사용자에 게는 1gb 네트워크 어댑터 및 RSS (수신측 배율) 지원을 사용 하는 것이 좋습니다. 이 시나리오의 유효성을 검사 하려면 특히 서버 통합 시나리오의 경우 \* 사이트의 모든 dc에서 네트워크 인터페이스 () \ 바이트/초를 확인 하 고,이를 함께 추가 하 고, 대상 도메인 컨트롤러 수로 나누어 적절 한 용량이 있는지 확인 합니다. 이 작업을 수행 하는 가장 쉬운 방법은 Windows 안정성 및 성능 모니터 (이전에는 Perfmon 이라고 함)에서 "누적 영역" 보기를 사용 하 여 모든 카운터가 동일 하 게 확장 되도록 하는 것입니다.

다음 예제를 고려 합니다 .이는 일반적으로 일반 규칙이 특정 환경에 적용 되는지 확인 하는 매우 복잡 한 방법입니다. 다음과 같은 가정이 적용 됩니다.

- 목표는 최대한 적은 수의 서버로 공간을 줄이는 것입니다. 이상적으로는 한 서버에서 부하를 수행 하 고 추가 서버를 중복성 (*N* + 1 시나리오)에 배포 합니다.
- 이 시나리오에서 현재 네트워크 어댑터는 100 MB만 지원 하며 전환 된 환경에 있습니다.
  N 개 시나리오에서 최대 대상 네트워크 대역폭 사용률은 60%입니다 (DC 손실).
- 각 서버에는 약 1만 클라이언트가 연결 되어 있습니다.

차트의 데이터에서 얻은 지식 (네트워크 인터페이스 ( \* ) \ 보낸 바이트/초):

1. 영업일 기준으로 5:30, 7:00 PM에 기간 동안 가동 중지 됩니다.
1. 최대 사용률이 가장 높은 기간은 오전 8:00에서 8:15 오전이 고, 최대 사용 DC에서 초당 25 바이트가 전송 됩니다.
   > [!NOTE]
   > 모든 성능 데이터를 기록 합니다. 따라서 8:15의 최대 데이터 요소는 8:00에서 8:15으로의 로드를 나타냅니다.
1. 서로 다른 표준 시간대 또는 백그라운드 인프라 작업 (예: 백업)에서 로드 하는 것을 나타낼 수 있는 가장 많은 DC에서 초당 20 바이트가 전송 되 고 오전 4:00 이전에 스파이크가 발생 합니다. 최대 8:00 AM은이 작업을 초과 하므로 관련이 없습니다.
1. 사이트에 5 개의 도메인 컨트롤러가 있습니다.
1. 최대 부하는 DC 당 약 5.5 m b/초 이며 100 MB 연결의 44%를 나타냅니다. 이 데이터를 사용 하는 경우 오전 8:00 시와 8:15 오전 사이에 필요한 총 대역폭은 28 m b/초입니다.
   > [!NOTE]
   > 네트워크 인터페이스 송신/수신 카운터는 바이트이 고 네트워크 대역폭은 비트로 측정 된다는 점에 주의 해야 합니다. 100 MB &divide; 8 = 12.5 mb, 1gb &divide; 8 = 128 MB

결론

1. 이 현재 환경은 60% 목표 사용률에 대 한 N + 1 수준의 내결함성을 충족 합니다. 한 시스템을 오프 라인으로 전환 하면 서버당 대역폭이 약 5.5 m b/초 (44%)로 이동 합니다. 약 7MB/s (56%)로
1. 이전에 설명한 것 처럼 한 서버에 통합 하는 목표를 기준으로이 둘 다 최대 대상 사용률을 초과 하 고 이론적으로 100 MB 연결을 사용할 수 있습니다.
1. 1GB 연결을 사용 하는 경우 총 용량의 22%가 표시 됩니다.
1. *N* + 1 시나리오의 정상 작동 조건에서 클라이언트 로드는 서버당 약 14mb/s 또는 총 용량의 11%에 상대적으로 균등 하 게 배포 됩니다.
1. DC를 사용할 수 없는 상태에서 충분 한 용량을 보장 하기 위해 서버당 정상적인 운영 대상은 약 30%의 네트워크 사용률 또는 서버당 38 m b/초입니다. 장애 조치 (Failover) 대상은 60%의 네트워크 사용률 또는 72 m b/초입니다.

즉, 시스템의 최종 배포에는 1GB 네트워크 어댑터가 있어야 하며,이에 대 한 부하를 지원할 네트워크 인프라에 연결 되어 있어야 합니다. 추가 정보는 생성 된 네트워크 트래픽 양이 지정 된 경우 네트워크 통신의 CPU 부하는 상당한 영향을 미칠 수 있으며 AD DS의 최대 확장성을 제한할 수 있습니다. 이 동일한 프로세스를 사용 하 여 DC에 대 한 인바운드 통신의 양을 예측할 수 있습니다. 그러나 인바운드 트래픽과 관련 된 아웃 바운드 트래픽의 predominance 대부분의 환경에 대 한 교육 과정입니다. 서버당 사용자가 5000 이상인 환경에서는 RSS에 대 한 하드웨어 지원이 중요 합니다. 네트워크 트래픽이 높은 시나리오의 경우 인터럽트 부하 분산이 병목 상태가 될 수 있습니다. 이는 프로세서 ( \* ) \% 인터럽트 시간이 cpu 전체에 분산 균일 하지 않게 하는 것을 감지할 수 있습니다. RSS 사용 Nic는 이러한 제한을 완화 하 고 확장성을 높일 수 있습니다.

> [!NOTE]
> 비슷한 방법을 사용 하 여 데이터 센터를 통합 하거나 위성 위치에서 도메인 컨트롤러를 사용 중지 하는 데 필요한 추가 용량을 예상할 수 있습니다. 클라이언트에 대 한 아웃 바운드 및 인바운드 트래픽을 수집 하기만 하면 이제 WAN 링크에 표시 되는 트래픽 양이 됩니다.
>
> 경우에 따라 인증서 확인이 WAN에서 적극적 시간 초과를 충족 하지 못하는 경우와 같이 트래픽이 느리기 때문에 예상 보다 많은 트래픽이 발생할 수 있습니다. 이러한 이유로 WAN 크기 조정 및 사용률은 반복적인 지속적인 프로세스 여야 합니다.

### <a name="virtualization-considerations-for-network-bandwidth"></a>네트워크 대역폭에 대 한 가상화 고려 사항

물리적 서버에 대 한 권장 사항은 5000 보다 큰 사용자를 지 원하는 서버에 대해 1gb를 사용 하는 것이 쉽습니다. 여러 게스트가 기본 가상 스위치 인프라를 공유 하기 시작 하면 시스템에서 모든 게스트를 지원할 수 있도록 호스트에 적절 한 네트워크 대역폭이 있는지 확인 하는 추가 주의가 필요 하므로 추가 데 어려움이 있습니다 필요 합니다. 이는 호스트 컴퓨터에 네트워크 인프라를 확인 하는 확장에 대 한 것입니다. 네트워크 트래픽이 가상 스위치를 통과 하는 네트워크 트래픽이 있는 호스트에서 가상 컴퓨터 게스트로 실행 되는 도메인 컨트롤러를 포함 하는지 여부 또는 실제 스위치에 직접 연결 되었는지 여부에 관계 없이 사용 됩니다. 가상 스위치는 업링크에서 전송 되는 데이터의 양을 지원 해야 하는 구성 요소가 하나 뿐입니다. 따라서 스위치에 연결 된 실제 호스트 실제 네트워크 어댑터는 DC 로드와 실제 네트워크 어댑터에 연결 된 가상 스위치를 공유 하는 다른 모든 게스트를 지원할 수 있어야 합니다.

### <a name="calculation-summary-example"></a>계산 요약 예

|시스템|최대 대역폭|
|-|-|
DC 1|6.5 m b/초|
DC 2|6.25 m b/초|
|DC 3|6.25 m b/초|
|DC 4|5.75 m b/초|
|DC 5|4.75 m b/초|
|합계|28.5 m b/초|

**권장: 72 m b/초** (28.5 m b/초를 40%로 구분)

|대상 시스템 수|총 대역폭 (위에서)|
|-|-|
|2|28.5 m b/초|
|결과 정상 동작|28.5 &divide; 2 = 14.25 m b/초|

시간이 지남에 따라 클라이언트 부하가 증가 하 고 가능한 한 최적으로 이러한 성장을 계획 해야 할 수 있습니다. 계획에 권장 되는 크기는 50%의 네트워크 트래픽 예상 증가를 허용 합니다.

## <a name="storage"></a>스토리지

저장소 계획은 다음 두 가지 구성 요소로 구성 됩니다.

- 용량 또는 저장소 크기
- 성능

용량 계획에는 상당한 시간 및 설명서가 소비 되므로 성능이 완전히 간과 되는 경우가 많습니다. 현재 하드웨어 비용을 고려 하 여 대부분의 환경에서는 이러한 두 가지 환경 중 하나를 실제로 고려 하지 않아도 되 고 "데이터베이스 크기 만큼 RAM을 배치" 하는 것이 더 큰 환경에서 위성 위치를 사용 하는 데 과도 한 영향을 미칠 수 있는 것이 좋습니다.

### <a name="sizing"></a>크기 조정

#### <a name="evaluating-for-storage"></a>저장소 평가

13 년 전까지 Active Directory가 도입 된 시간에 비해 4gb와 9GB 드라이브가 가장 일반적인 드라이브 크기인 경우에는 Active Directory 크기를 조정 하는 것이 가장 큰 환경에서는 고려 하지 않아도 됩니다. 180 GB 범위에서 사용 가능한 가장 작은 하드 드라이브 크기를 사용 하는 경우 전체 운영 체제, SYSVOL 및 ntds.dit를 한 드라이브에 쉽게 맞출 수 있습니다. 따라서이 영역에 많은 투자를 사용 중단 것이 좋습니다.

유일한 고려 사항은 조각 모음을 사용 하도록 설정 하기 위해 ntds.dit 크기의 110%를 사용할 수 있는지 확인 하는 것입니다. 또한 하드웨어 수명에 대 한 성장을 적절 해야 합니다.

첫 번째 및 가장 중요 한 고려 사항은 ntds.dit와 SYSVOL의 크기를 평가 하는 것입니다. 이러한 측정값은 고정 디스크 및 RAM 할당의 크기를 조정 합니다. 이러한 구성 요소에 대 한 비용 절감 (상대적)으로 인해 수학은 엄격 하 고 정확 하지 않아도 됩니다. 기존 환경 및 새 환경 모두에 대해이를 평가 하는 방법에 대 한 내용은 [데이터 저장소](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/cc961771(v=technet.10)) 시리즈의 문서에서 찾을 수 있습니다. 구체적으로 다음 문서를 참조 하세요.

- **기존 환경의 &ndash; 경우** 문서 [저장소 제한](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/cc961769(v=technet.10))문서에서 "조각 모음이 해제 된 디스크 공간의 로깅을 활성화 하려면" 섹션을 참조 하세요.
- **새 환경의 &ndash; 경우** [Active Directory 사용자 및 조직 구성 단위에 대 한 증가 예측](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/cc961779(v=technet.10))이라는 문서입니다.

  > [!NOTE]
  > 이 문서는 Windows 2000의 Active Directory 릴리스가 출시 될 때 예상 되는 데이터 크기를 기준으로 합니다. 사용자 환경에서 개체의 실제 크기를 반영 하는 개체 크기를 사용 합니다.

여러 도메인이 있는 기존 환경을 검토할 때 데이터베이스 크기에 차이가 있을 수 있습니다. 이 값이 true 이면 가장 작은 GC (글로벌 카탈로그) 및 비 GC 크기를 사용 합니다.

데이터베이스 크기는 운영 체제 버전에 따라 다를 수 있습니다. Windows Server 2003와 같은 이전 버전의 운영 체제를 실행 하는 dc는 Windows Server 2008 r 2와 같은 이후 운영 체제를 실행 하는 DC 보다 더 작은 데이터베이스 크기를 사용 합니다. 특히 휴지통 또는 자격 증명 로밍을 사용 하는 경우에 Active Directory 합니다.

> [!NOTE]
  >
>- 새 환경의 경우 Active Directory 사용자 및 조직 구성 단위에 대 한 예상 예상 값은 10만 사용자 (동일한 도메인)가 450 MB의 공간을 사용 함을 의미 합니다. 채워진 특성은 총 금액에 큰 영향을 줄 수 있습니다. 특성은 Microsoft Exchange Server 및 Lync를 비롯 한 타사 및 Microsoft 제품에 의해 많은 개체에 채워집니다. 환경에 있는 제품의 포트폴리오를 기반으로 하는 평가를 사용 하는 것이 좋습니다. 하지만 수학을 자세히 설명 하 고 가장 큰 환경에 대 한 정확한 추정치를 테스트 하는 연습은 실제로 상당한 시간과 노력이 필요 하지 않을 수 있습니다.
>- 오프 라인 조각 모음을 사용 하도록 설정 하기 위해 ntds.dit 크기의 110%를 여유 공간으로 사용할 수 있는지 확인 하 고 3 ~ 5 년간의 하드웨어 수명 동안 성장을 계획 합니다. 저장소에 대 한 비용을 계산 하는 것은 저장소 할당의 저장소를 300 예상 하는 것이 안전 합니다.

#### <a name="virtualization-considerations-for-storage"></a>저장소에 대 한 가상화 고려 사항

단일 볼륨에 여러 VHD (가상 하드 디스크) 파일을 할당 하는 시나리오에서, 예약 된 공간을 확보 하기 위해 DIT 크기의 210% (DIT + 110% 사용 가능한 공간의 100%)의 고정 크기 디스크를 사용 합니다.

#### <a name="calculation-summary-example"></a>계산 요약 예

|평가 단계에서 수집 된 데이터| |
|-|-|
|Ntds.dit 크기|3.5GB|
|오프 라인 조각 모음을 허용 하는 한정자|2.1|
|필요한 총 저장소|73.5 GB|

> [!NOTE]
> 이 저장소는 SYSVOL, 운영 체제, 페이지 파일, 임시 파일, 로컬에 캐시 된 데이터 (예: 설치 관리자 파일) 및 응용 프로그램에 필요한 저장소에 추가 해야 합니다.

### <a name="storage-performance"></a>스토리지 성능

#### <a name="evaluating-performance-of-storage"></a>저장소 성능 평가

모든 컴퓨터 내의 가장 느린 구성 요소인 저장소는 클라이언트 환경에 가장 심각한 영향을 미칠 수 있습니다. RAM 크기 조정 권장 사항이 적합 하지 않은 환경의 경우 overlooking plan for performance의 결과는 매우 클 수 있습니다.  또한 복잡 하 고 다양 한 유형의 저장소 기술은 별도의 물리적 디스크에 "운영 체제, 로그 및 데이터베이스 배치"에 대 한 장기간의 모범 사례와 관련 된 유용한 시나리오를 비롯 하 여 오류 위험을 더욱 증가 시킵니다.  이는 "디스크"가 전용 스핀 들이 고이를 격리 시킬 수 있다는 가정을 기반으로 하는 장기 모범 사례입니다.  이를 true로 설정 하는 것은 더 이상 다음을 소개 하는 것과 관련이 없습니다.

- 새로운 저장소 유형 및 가상화 및 공유 저장소 시나리오
- SAN (저장 영역 네트워크)의 공유 스핀 들
- SAN 또는 네트워크에 연결 된 저장소의 VHD 파일
- 반도체 드라이브
- 계층화 된 저장소 아키텍처 (즉, 더 큰 스핀 들 기반 저장소를 캐시 하는 SSD 저장소 계층)

간단히 말해서 기본 저장소 아키텍처와 디자인에 관계 없이 모든 저장소 성능 노력의 목표는 필요한 IOPS (초당 입력/출력 작업 수)를 사용할 수 있는지와 해당 IOPS가 허용 되는 시간 프레임 내에 발생 하는지 확인 하는 것입니다. 이 섹션에서는 저장소 솔루션이 적절히 디자인 되도록 하기 위해 기본 저장소의 AD DS 요구 사항을 평가 하는 방법을 설명 합니다.  오늘날의 저장소 기술의 가변성을 감안 하 여 저장소 공급 업체와 협력 하 여 적절 한 IOPS를 확인 하는 것이 가장 좋습니다.  로컬로 연결 된 저장소를 사용 하는 시나리오의 경우 기존 로컬 저장소 시나리오를 디자인 하는 방법의 기본 사항에 대해서는 부록 C를 참조 하세요.  이 보안 주체는 일반적으로 더 복잡 한 저장소 계층에 적용 되며, 백 엔드 저장소 솔루션을 지 원하는 공급 업체와 함께 대화에도 도움이 됩니다.

- 광범위 한 저장소 옵션을 사용할 수 있는 경우 하드웨어 지원 팀 또는 공급 업체의 전문 지식을 활용 하 여 특정 솔루션이 AD DS 요구를 충족 하는지 확인 하는 것이 좋습니다. 저장소 전문가에 게 제공 되는 정보는 다음과 같습니다.

데이터베이스가 너무 커서 RAM에 저장할 수 없는 환경의 경우에는 성능 카운터를 사용 하 여 지원 되는 i/o의 양을 결정 합니다.

- 논리 디스크 ( \* ) \Avg Disk sec/Read (예: ntds.dit가 D:/에 저장 된 경우) 드라이브, 전체 경로는 논리 디스크 (D: \Avg Disk sec/Read)입니다.
- 논리 디스크 ( \* ) \Avg Disk sec/Write
- 논리 디스크 ( \* ) \Avg Disk sec/Transfer
- 논리 디스크 ( \* ) \ 읽기/초
- 논리 디스크 ( \* ) \ 쓰기/초
- 논리 디스크 ( \* ) \ 전송/초

현재 환경의 요구 사항을 벤치 마크 위해 15/30/60 분 간격으로 샘플링 되어야 합니다.

#### <a name="evaluating-the-results"></a>결과 평가

> [!NOTE]
> 이는 데이터베이스에서 읽기에 대 한 것으로, 일반적으로 가장 까다로운 구성 요소 이기 때문에 논리 디스크 ( *\<NTDS Log\>* ) \Avg Disk sec/Write 및 논리 디스크 ( *\<NTDS Log\>* ) \ Write/sec를 대체 하 여 로그 파일에 쓸 때와 동일한 논리를 적용할 수 있습니다.
>
> - 논리 디스크 ( *\<NTDS\>* ) \Avg Disk sec/Read는 현재 저장소의 크기를 적절 하 게 조정 하는지 여부를 나타냅니다.  결과가 디스크 유형에 대 한 디스크 액세스 시간과 거의 같으면 논리 디스크 ( *\<NTDS\>* ) \ Reads/sec는 유효한 측정값입니다.  백 엔드에서 저장소에 대 한 제조업체 사양을 확인 하지만 논리 디스크 ( *\<NTDS\>* ) \Avg Disk sec/Read에 대 한 좋은 범위는 대략적으로 다음과 같습니다.
>   - 7200 – 9 ~ 12.5 밀리초 (밀리초)
>   - 1만 – 6-10 밀리초
>   - 15000 – 4-6ms
>   - SSD – 1 ~ 3 밀리초
>   - > [!NOTE]
>     > 저장소 성능이 15ms에서 20ms (원본에 따라 다름)로 저하 됨을 나타내는 권장 사항이 있습니다.  위의 값과 다른 지침의 차이는 위의 값이 정상 작동 범위 임을 나타내는 것입니다.  다른 권장 사항은 클라이언트 환경이 현저 하 게 저하 되 고 눈에 띄는 경우를 식별 하기 위한 문제 해결 지침입니다.  자세히 설명 하는 부록 C를 참조 하세요.
> - 논리 디스크 ( *\<NTDS\>* ) \ Reads/sec는 수행 중인 i/o의 양입니다.
>   - 논리 디스크 ( *\<NTDS\>* ) \Avg Disk sec/Read가 백 엔드 저장소에 대 한 최적의 범위 내에 있는 경우 논리 디스크 ( *\<NTDS\>* ) \ Reads/sec를 직접 사용 하 여 저장소 크기를 지정할 수 있습니다.
>   - 논리 디스크 ( *\<NTDS\>* ) \Avg Disk sec/Read가 백 엔드 저장소에 대 한 최적의 범위 내에 있지 않은 경우 다음 수식에 따라 추가 i/o가 필요 합니다.
>     > (논리 디스크 ( *\<NTDS\>* ) \Avg Disk sec/Read) &divide; (실제 미디어 디스크 액세스 시간) &times; (논리 디스크 ( *\<NTDS\>* ) \Avg Disk sec/Read)

고려 사항:

- 서버가 최적의 RAM 크기를 사용 하 여 구성 된 경우 이러한 값은 계획 목적으로 정확 하지 않습니다.  이는 매우 중요 하지 않으며 최악의 시나리오로 사용할 수 있습니다.
- RAM을 구체적으로 추가/최적화 하면 읽기 i/o (논리 디스크 ( *\<NTDS\>* ) \ Reads/Sec의 양이 감소 합니다.  즉, 저장소 솔루션은 초기에 계산 된 것 만큼 강력 하지 않을 수 있습니다.  불행 하 게이 일반적인 문 보다 구체적인 항목은 클라이언트 부하에 따라 environmentally 일반적인 지침을 제공할 수 없습니다.  최상의 옵션은 RAM을 최적화 한 후 저장소 크기 조정을 조정 하는 것입니다.

#### <a name="virtualization-considerations-for-performance"></a>성능에 대 한 가상화 고려 사항

앞의 모든 가상화 토론과 마찬가지로, 기본 공유 인프라에서 기본 공유 미디어를 사용 하는 다른 리소스와이에 대 한 모든 경로를 사용 하 여 DC 부하를 지원할 수 있도록 하는 것이 중요 합니다. 실제 도메인 컨트롤러는 SAN, nas 또는 iSCSI 인프라의 동일한 기본 미디어를 다른 서버 또는 응용 프로그램으로 공유 하는 경우, 즉, 기본 미디어를 공유 하는 SAN, NAS 또는 iSCSI 인프라에 대 한 통과 액세스를 사용 하는 게스트 인지 또는 게스트에서 로컬 또는 SAN 공유 미디어에 있는 VHD 파일을 사용 하는 경우에도 마찬가지입니다. , NAS 또는 iSCSI 인프라. 계획 연습은 기본 미디어가 모든 소비자의 총 부하를 지원할 수 있는지 확인 하는 것입니다.

또한 게스트 관점에서 트래버스 해야 할 추가 코드 경로가 있으므로 호스트를 통해 저장소에 액세스 해야 하는 성능에 영향을 줄 수 있습니다. 저장소 성능 테스트는 가상화가 호스트 시스템의 프로세서 사용률에 대 한 주관적인 처리량에 영향을 주는지 여부를 나타냅니다 (부록 A: CPU 크기 조정 기준 참조) .이는 게스트에서 요구 하는 호스트의 리소스에 의해 영향을 받습니다. 이는 가상화 된 시나리오에서 처리 요구 사항과 관련 된 가상화 고려 사항에 기여 합니다 ( [처리를 위한 가상화 고려 사항](#virtualization-considerations-for-processing)참조).

좀 더 복잡 하 게 만드는 것은 성능에 영향을 주는 다양 한 저장소 옵션을 사용할 수 있다는 것입니다. 물리적에서 가상으로 마이그레이션하는 경우 안전 예상치로, 승수 1.10를 사용 하 여 통과 저장소, SCSI 어댑터 또는 IDE와 같은 Hyper-v의 가상화 된 게스트에 대 한 다양 한 저장소 옵션을 조정 합니다. 서로 다른 저장소 시나리오 간에 전송할 때 수행 해야 하는 조정은 저장소가 로컬, SAN, NAS 또는 iSCSI 인지와 관련이 없습니다.

#### <a name="calculation-summary-example"></a>계산 요약 예

정상적인 운영 상태에서 정상 시스템에 필요한 i/o의 양 결정:

- 최대 사용 *\<NTDS Database Drive\>* 기간 (15 분) 동안의 논리 디스크 () \ 전송/초
- 기본 저장소의 용량을 초과 하는 저장소에 필요한 i/o 용량을 확인 하려면 다음을 수행 합니다.
  >*필요한 IOPS* = (논리 디스크 ( *\<NTDS Database Drive\>* ) \avg Disk sec/Read &divide; *\<Target Avg Disk sec/Read\>* ) &times; 논리 디스크 ( *\<NTDS Database Drive\>* ) \ 읽기/초

|카운터|값|
|-|-|
|실제 논리 디스크 ( *\<NTDS Database Drive\>* ) \Avg Disk sec/Transfer|02 초 (20 밀리초)|
|Target 논리 디스크 ( *\<NTDS Database Drive\>* ) \Avg Disk sec/Transfer|.01 초|
|사용 가능한 IO 변경에 대 한 승수|0.02 &divide; 0.01 = 2|

|값 이름|값|
|-|-|
|논리 디스크 ( *\<NTDS Database Drive\>* ) \ 전송/초|400|
|사용 가능한 IO 변경에 대 한 승수|2|
|피크 기간 동안 필요한 총 IOPS|800|

캐시를 준비 하는 속도를 결정 하려면 다음을 수행 합니다.

- 캐시를 준비 하는 데 허용 되는 최대 시간을 결정 합니다. 디스크에서 전체 데이터베이스를 로드 하는 데 소요 되는 시간 이거나 전체 데이터베이스를 RAM에 로드할 수 없는 시나리오의 경우 RAM을 채울 수 있는 최대 시간입니다.
- 공백을 제외 하 고 데이터베이스의 크기를 확인 합니다.  자세한 내용은 [저장소에 대 한 평가](#evaluating-for-storage)를 참조 하세요.
- 데이터베이스 크기를 8kb로 나눕니다. 이는 데이터베이스를 로드 하는 데 필요한 총 Io입니다.
- 총 Io 수를 정의 된 시간 프레임의 시간 (초)으로 나눕니다.

ESE가 고정 캐시 크기를 갖도록 구성 되지 않은 경우에는 정확 하 게 계산 된 비율이 정확 하지 않으며, 기본적으로 AD DS는 변수 캐시 크기를 사용 합니다.

|수집할 데이터 요소|값
|-|-|
|웜 대기에 허용 되는 최대 시간|10 분 (600 초)
|데이터베이스 크기|2GB|

|계산 단계|Formula|결과|
|-|-|-|
|페이지의 데이터베이스 크기 계산|(2gb &times; 1024 &times; 1024) = *데이터베이스 크기 (KB* )|2097152 KB|
|데이터베이스의 페이지 수 계산|2097152 KB &divide; 8kb = *페이지 수*|262144 페이지|
|캐시를 완전히 준비 하는 데 필요한 IOPS 계산|262144 페이지 &divide; 600 초 = *IOPS 필요*|437 IOPS|

## <a name="processing"></a>처리 중

### <a name="evaluating-active-directory-processor-usage"></a>Active Directory 프로세서 사용 평가

대부분의 환경에서 저장소, RAM 및 네트워킹은 계획 섹션에 설명 된 대로 적절히 조정 된 후 처리 용량의 크기를 관리 하는 것이 가장 중요 한 구성 요소가 됩니다. 필요한 CPU 용량을 평가 하는 데는 두 가지 과제가 있습니다.

- 환경의 응용 프로그램을 공유 서비스 인프라에서 제대로 작동 하 고 있는지 여부와 상관 없이, 보다 효율적인 Microsoft Active Directory 지원 응용 프로그램을 만드는 방법에 대 한 "비용이 많이 들고 비효율적인 검색 추적" 섹션에서 설명 하 고 LDAP 호출에 대 한 하위 수준 SAM 호출에서 더 이상 마이그레이션할 수 있습니다.

  대규모 환경에서는 잘못 된 코딩 된 응용 프로그램이 CPU 로드에서 변동성을 구동 하 고, 다른 응용 프로그램에서 과도의 CPU 시간을 "도용" 하 고, 용량 요구 사항을 인위적으로 수행 하 고, Dc에 대 한 부하를 분산 하는 것이 중요 합니다.
- AD DS은 다양 한 클라이언트를 사용 하는 분산 환경 이므로 "단일 클라이언트"의 비용을 예측 하는 것은 사용 패턴 및 AD DS 활용 하는 응용 프로그램의 유형 또는 수량으로 인해 주관적인 environmentally. 즉, 네트워킹 섹션과 매우 유사 하 게 광범위 한 적용 가능성을 위해 환경에 필요한 총 용량을 평가 하는 관점에서 더 나은 접근 방식을 사용할 수 있습니다.

기존 환경에서 저장소 크기 조정을 이전에 설명 했으므로 이제 저장소 크기를 적절히 조정 하 여 프로세서 부하와 관련 된 데이터가 유효한 것으로 가정 합니다. 다시 말해, 시스템 병목 현상이 저장소의 성능이 아님을 확인 하는 것이 중요 합니다. 병목 상태가 발생 하 고 프로세서가 대기 중인 경우 병목 상태가 제거 되 면 대기 상태가 됩니다.  프로세서 대기 상태가 제거 되 면 정의에 의해 CPU 사용률이 더 이상 데이터를 기다릴 필요가 없으므로 늘어납니다. 따라서 성능 카운터 "논리 디스크 ( *\<NTDS Database Drive\>* ) \Avg disk sec/Read" 및 "Process (lsass) \\ % Processor Time"을 수집 합니다. " \\ 논리 디스크 ( *\<NTDS Database Drive\>* ) \avg disk Sec/Read"가 10 ~ 15 밀리초를 초과 하는 경우 "프로세스 (lsass)% Processor Time"의 데이터는 인위적으로 낮으므로 Microsoft 지원에서 저장소 관련 성능 문제를 해결 하는 데 사용 하는 일반 임계값입니다. 앞서와 같이 샘플 간격은 15, 30 또는 60 분이 되도록 하는 것이 좋습니다. 더 작은 것은 일반적으로 적절 한 측정을 위해 너무 휘발성입니다. 더 큰 것은 매일 피킹을 과도 하 게 완만 하 게 됩니다.

### <a name="introduction"></a>소개

도메인 컨트롤러에 대 한 용량 계획을 계획 하기 위해 처리 능력에는 가장 많은 주의가 필요 합니다. 최대 성능을 보장 하기 위해 시스템의 크기를 조정할 때 병목 상태의 구성 요소가 항상 있으며 적절 한 크기의 도메인 컨트롤러에서이는 프로세서가 됩니다.

사이트 별로 환경의 수요를 검토 하는 네트워킹 섹션과 마찬가지로, 요청 된 계산 용량의 경우에도 동일 하 게 수행 해야 합니다. 사용 가능한 네트워킹 기술이 정상적인 수요를 초과 하는 네트워킹 섹션과 달리 CPU 용량 크기 조정에 더 주의를 기울여야 합니다.  중간 규모의 모든 환경으로 수천 명의 동시 사용자에 대 한 모든 항목은 CPU에 상당한 부하를 넣을 수 있습니다.

불행 하 게도 AD를 활용 하는 클라이언트 응용 프로그램의 가변성 때문에 CPU 당 일반적으로 CPU 당 사용자 예상치는 심각할 모든 환경에 적용할 수 없습니다. 특히 계산 요구는 사용자 동작 및 응용 프로그램 프로필의 영향을 받습니다. 따라서 각 환경의 크기를 개별적으로 지정 해야 합니다.

#### <a name="target-site-behavior-profile"></a>대상 사이트 동작 프로필

앞에서 설명한 것 처럼 전체 사이트에 대 한 용량을 계획할 때 목표는 *N* + 1 용량 디자인을 사용 하 여 디자인을 대상으로 하는 것입니다 .이를 위해 최대 사용 기간 동안 한 시스템의 장애가 발생 하면 적절 한 품질 수준에서 서비스를 계속 사용할 수 있습니다. 즉, "*N*" 시나리오에서 모든 상자를 로드 하는 것은 100% 미만 이어야 합니다 (더 잘, 80% 미만). 최대 사용 기간 중.

또한 사이트의 응용 프로그램 및 클라이언트에서 도메인 컨트롤러를 찾는 데 모범 사례를 사용 하는 경우 (즉, [DsGetDcName 함수](https://docs.microsoft.com/windows/win32/api/dsgetdc/nf-dsgetdc-dsgetdcnamea)사용), 클라이언트는 여러 요소로 인해 약간의 일시적인 급증을 통해 비교적 고르게 분산 되어야 합니다.

다음 예제에서는 다음과 같은 가정이 적용 됩니다.

- 사이트에서 5 개의 각 Dc에는 4 개의 Cpu가 있습니다.
- 업무 시간 동안의 총 목표 CPU 사용량은 정상 운영 조건 ("*n* + 1")에서 40%이 60 고, 그렇지 않으면 ("*n*")입니다. 업무 시간 외 시간에는 백업 소프트웨어 및 기타 유지 관리가 사용 가능한 모든 리소스를 사용 해야 하므로 대상 CPU 사용량은 80%입니다.

![CPU 사용량 차트](media/capacity-planning-considerations-cpu-chart.png)

각 Dc에 대해 차트에서 데이터를 분석 합니다 `(Processor Information(_Total)\% Processor Utility)` .

- 대부분의 경우에는 클라이언트가 DC 로케이터를 사용 하 고 잘 작성 된 검색을 사용할 때 예상 되는 부하가 비교적 고르게 분산 됩니다.
- 10%의 5 분 급증 횟수가 있으며, 그 중 일부는 20%입니다. 일반적으로 용량 계획 대상을 초과 하지 않는 한이를 조사 하는 것은 의미가 없습니다.
- 모든 시스템의 최고 사용 기간은 약 오전 8:00 시와 9:15 오전 사이입니다. 약 5:00 AM부터 약 5:00 PM까지 원활한 전환을 통해이는 일반적으로 비즈니스 주기를 나타냅니다. 5:00 PM과 4:00 오전 사이에 있는 box 시나리오에서 CPU 사용량이 무작위로 급증 하는 것은 용량 계획 문제를 벗어납니다.

  > [!NOTE]
  > 잘 관리 되는 시스템에서 급증은 백업 소프트웨어 실행, 전체 시스템 바이러스 백신 검사, 하드웨어 또는 소프트웨어 인벤토리, 소프트웨어 또는 패치 배포 등이 될 수 있습니다. 이러한 사용자는 최고 사용자 비즈니스 주기를 벗어나므로 대상이 초과 되지 않습니다.

- 각 시스템의 Cpu 수가 40%이 고 모든 시스템에 동일한 수의 Cpu가 있는 경우, 하나는 실패 하거나 오프 라인으로 전환 됩니다. 나머지 시스템은 예상 53% (시스템 D의 40% 부하가 고르게 분할 되 고 시스템 A의 기존 40% load에 추가 됨)에 실행 됩니다. 여러 가지 이유로이 선형 가정은 완벽 하 게 정확 하지는 않지만 계기에 충분 한 정확도를 제공 합니다.

  **대체 시나리오-** 40%에서 실행 되는 두 도메인 컨트롤러: 하나의 도메인 컨트롤러가 실패 하 여 남은 CPU의 예상 CPU가 80%가 됩니다. 이는 용량 계획에 대해 위에서 설명한 임계값을 초과 하 고 위의 부하 프로필에 표시 되는 10% ~ 20%의 헤드 공간을 심각 하 게 제한 하기 시작 합니다. 즉, "*N*" 시나리오에서 급증 시 DC를 90%에서 100%로 설정 하 고 응답성이 저하 됩니다.

### <a name="calculating-cpu-demands"></a>CPU 수요 계산

"Process \\ % Processor Time" 성능 개체 카운터는 응용 프로그램의 모든 스레드가 CPU에서 소비 하는 총 시간을 합산 하 여 경과 된 시스템 시간의 총 크기를 나눕니다. 이로 인해 다중 CPU 시스템의 다중 스레드 응용 프로그램이 100%의 CPU 시간을 초과 하 여 "프로세서 정보 \\ % Processor Utility"와 매우 다르게 해석 될 수 있습니다. 실제로 프로세스 \\ 의 요청을 지 원하는 데 필요한 100%에서 실행 되는 cpu 수로 "프로세스 (lsass)% 프로세서 시간"을 볼 수 있습니다. 값이 200% 이면 전체 AD DS 로드를 지원 하기 위해 각각 100%의 Cpu 2 개를 사용 해야 합니다. 100% 용량으로 실행 되는 CPU는 Cpu 및 전력 및 에너지 소비에 대 한 비용 측면에서 가장 비용 효율적 이지만 부록 A에 자세히 설명 된 여러 가지 이유로 인해 시스템이 100%에서 실행 되지 않는 경우 다중 스레드 시스템에서 응답성이 향상 됩니다.

클라이언트 부하의 일시적인 급증을 수용 하기 위해 시스템 용량의 40%에서 60% 사이인 최대 CPU CPU를 대상으로 하는 것이 좋습니다. 위의 예제에서 작업 하는 경우 AD DS (lsass 프로세스) 로드에 3.33 (60% target) 및 5 (40% target) Cpu가 필요 함을 의미 합니다. 기본 운영 체제 및 필요한 기타 에이전트 (예: 바이러스 백신, 백업, 모니터링 등)의 요구에 따라에 추가 용량을 추가 해야 합니다. 환경 마다 에이전트의 영향을 평가 해야 하지만 단일 CPU의 5%에서 10% 사이의 예상 값을 만들 수 있습니다. 현재 예제에서는 최대 기간 동안 3.43 (60% target) 및 5.1 (40% target) Cpu가 필요 하다는 것을 제안 합니다.

이 작업을 수행 하는 가장 쉬운 방법은 Windows 안정성 및 성능 모니터 (perfmon)에서 "누적 영역형" 보기를 사용 하 여 모든 카운터가 동일 하 게 확장 되도록 하는 것입니다.

가정:

- 목표는 최대한 적은 수의 서버로 공간을 줄이는 것입니다. 이상적으로는 한 서버에서 로드를 수행 하 고 중복성 (*N* + 1 개 시나리오)에 추가 서버를 추가 합니다.

![Lsass 프로세스에 대 한 프로세서 시간 차트 (모든 프로세서)](media/capacity-planning-considerations-proc-time-chart.png)

차트의 데이터에서 얻은 정보 (프로세스 (lsass) \\ % 프로세서 시간):

- 영업일은 7:00을 기준으로 램프를 시작 하 고 5:00 PM에 감소 합니다.
- 최대 사용량이 가장 많은 기간은 오전 9:30에서 11:00 오전입니다.
  > [!NOTE]
  > 모든 성능 데이터를 기록 합니다. 9:15의 최고 데이터 요소는 9:00에서 9:15 까지의 부하를 나타냅니다.
- 다른 표준 시간대 또는 백그라운드 인프라 작업 (예: 백업)에서 로드를 나타낼 수 있는 7:00 오전 5 시 급증 합니다. 최대 9:30 AM은이 작업을 초과 하므로 관련이 없습니다.
- 사이트에는 세 개의 도메인 컨트롤러가 있습니다.

최대 로드에서 lsass는 한 CPU의 485% 또는 100%에서 실행 되는 4.85 Cpu를 사용 합니다. 이는 앞의 수치와 같이 사이트에 AD DS에 대 한 약 12.25 Cpu가 필요 함을 의미 합니다. 백그라운드 프로세스의 경우 위의 제안에 5% ~ 10%를 추가 하 고, 현재 서버를 교체 하는 것은 동일한 로드를 지원 하기 위해 약 12.30 ~ 12.35 Cpu가 필요 합니다. 이제 성장을 위한 환경 예상치는에서 팩터링 해야 합니다.

### <a name="when-to-tune-ldap-weights"></a>LDAP 가중치를 조정 하는 경우

튜닝 [LdapSrvWeight](https://docs.microsoft.com/previous-versions/windows/it-pro/windows-2000-server/cc957291(v=technet.10)) 을 고려해 야 하는 몇 가지 시나리오가 있습니다. 용량 계획의 컨텍스트 내에서이 작업은 응용 프로그램 또는 사용자 로드가 균등 하 게 분산 되지 않았거나 기본 시스템이 기능 측면에서 균등 하 게 분산 되지 않은 경우에 수행 됩니다. 용량 계획을 벗어나는 이유는이 문서의 범위를 벗어납니다.

LDAP 가중치를 조정 하는 두 가지 일반적인 이유는 다음과 같습니다.

- PDC 에뮬레이터는 사용자 또는 응용 프로그램 로드 동작이 균등 하 게 분산 되지 않는 모든 환경에 영향을 주는 예제입니다. PDC 에뮬레이터를 대상으로 하는 특정 도구 및 동작 (예: 그룹 정책 관리 도구, 인증 실패의 경우 두 번째 시도, 트러스트 설정 등)을 대상으로 하는 경우 PDC 에뮬레이터의 CPU 리소스가 사이트의 다른 위치 보다 더 많은 요구를 받을 수 있습니다.
  - PDC 에뮬레이터에 대 한 부하를 줄이고 다른 도메인 컨트롤러에 대 한 부하를 증가 시키기 위해 부하를 분산 시킬 수 있도록 CPU 사용률이 눈에 띄는 경우에만이를 조정 하는 것이 좋습니다.
  - 이 경우 PDC 에뮬레이터에 대해 LDAPSrvWeight를 50에서 75으로 설정 합니다.
- 사이트에서 Cpu (및 속도) 수가 서로 다른 서버  예를 들어 2 8 코어 서버와 1 4 코어 서버가 있다고 가정 합니다.  마지막 서버에는 다른 두 서버에 대 한 프로세서의 절반이 있습니다.  즉, 잘 분산 된 클라이언트 로드를 사용 하면 4 코어 상자의 평균 CPU 부하가 8 개 코어 상자의 약 두 배 정도 증가 합니다.
  - 예를 들어 2 8-core 상자는 40%에서 실행 되며 4 코어 상자는 80%에서 실행 됩니다.
  - 또한이 시나리오에서 1 8 코어 상자의 손실로 인 한 영향을 고려해 야 합니다. 특히 4 코어 상자가 오버 로드 될 수 있다는 사실입니다.

#### <a name="example-1---pdc"></a>예제 1-PDC

| |기본값 사용|새 LdapSrvWeight|예상 새 사용률|
|-|-|-|-|
|DC 1 (PDC 에뮬레이터)|53%|57|40%|
|DC 2|33%|100|40%|
|DC 3|33%|100|40%|

여기서는 PDC 에뮬레이터 역할을 전송 하거나 점유 하는 경우, 특히 사이트의 다른 도메인 컨트롤러에 대 한 새 PDC 에뮬레이터가 크게 증가 한다는 것을 알 수 있습니다.

[대상 사이트 동작 프로필](#target-site-behavior-profile)섹션의 예제를 사용 하 여 사이트의 세 도메인 컨트롤러 모두에 4 개의 cpu가 있다고 가정 합니다. 도메인 컨트롤러 중 하나에 8 개의 Cpu가 있는 경우 정상 상태에서 어떻게 되나요? 40% 사용률에는 두 개의 도메인 컨트롤러가 있으며 사용률은 20%입니다. 이는 나쁘지 않지만 부하를 약간 더 잘 조정할 수 있는 기회가 있습니다. LDAP 가중치를 활용 하 여이를 수행 합니다.  예제 시나리오는 다음과 같습니다.

#### <a name="example-2---differing-cpu-counts"></a>예제 2-CPU 수가 서로 다릅니다.

| |프로세서 정보 \\  % &nbsp; 프로세서 유틸리티 (_Total)<br />기본값 사용|새 LdapSrvWeight|예상 새 사용률|
|-|-|-|-|
|4-CPU DC 1|40|100|30%|
|4-CPU DC 2|40|100|30%|
|8-CPU DC 3|20|200|30%|

그러나 이러한 시나리오에는 주의 해야 합니다. 위에서 볼 수 있듯이, 수학은 매우 유용 하 고 멋진 용지를 보입니다. 그러나이 문서 전체에서 "*N* + 1" 시나리오를 계획 하는 것이 가장 중요 합니다. 모든 시나리오에 대해 하나의 DC가 오프 라인으로 전환 되는 영향은 계산 해야 합니다. 부하 분산을 사용 하는 즉시 이전 시나리오에서 "*N" 개*시나리오에서 60% 부하를 보장 하기 위해 모든 서버에서 부하가 균등 하 게 분산 되는 경우 비율이 일관 되 게 유지 됩니다. PDC 에뮬레이터 튜닝 시나리오를 살펴보면 일반적으로 사용자 또는 응용 프로그램 부하가 불균형 하는 시나리오에서 다음과 같은 결과가 매우 다릅니다.

| |튜닝 사용률|새 LdapSrvWeight|예상 새 사용률|
|-|-|-|-|
|DC 1 (PDC 에뮬레이터)|40%|85|47%|
|DC 2|40%|100|53%|
|DC 3|40%|100|53%|

### <a name="virtualization-considerations-for-processing"></a>처리를 위한 가상화 고려 사항

가상화 된 환경에서 수행 해야 하는 두 가지 용량 계획 계층이 있습니다. 호스트 수준에서 이전에 도메인 컨트롤러 처리에 대해 간략히 설명 된 비즈니스 주기의 id와 마찬가지로 사용량이 많은 기간 동안의 임계값을 식별 해야 합니다. 물리적 컴퓨터의 CPU에서 AD DS 스레드를 가져오기 위해 CPU에서 게스트 스레드를 예약 하는 호스트 컴퓨터에 대 한 기본 보안 주체가 동일 하기 때문에 기본 호스트에서 40% ~ 60%의 동일한 목표가 권장 됩니다. 다음 계층에서 게스트 계층은 스레드 일정의 보안 주체가 변경 되지 않았기 때문에 게스트의 목표가 40% ~ 60% 범위 내에 남아 있습니다.

직접 매핑된 시나리오에서 호스트 당 게스트 하나를 사용 하는 경우 기본 호스트 운영 체제의 요구 사항 (RAM, 디스크, 네트워크)에이 시점까지 수행 된 모든 용량을 추가 해야 합니다. 공유 호스트 시나리오에서 테스트는 기본 프로세서의 효율성에 10% 영향이 있음을 나타냅니다. 즉, 사이트에 40%의 대상에서 10 개의 Cpu가 필요한 경우 모든 "*N*" 게스트에서 할당할 수 있는 권장 되는 가상 cpu 양은 11입니다. 물리적 서버와 가상 서버를 혼합 하 여 배포 하는 사이트에서 한정자는 Vm에만 적용 됩니다. 예를 들어 사이트에 "*N* + 1" 시나리오가 있는 경우 10 개의 cpu가 있는 물리적 또는 직접 매핑된 서버 하나는 호스트에서 11 개 cpu가 있는 하나의 게스트와 동일 하 고 11 개의 cpu가 도메인 컨트롤러에 대해 예약 되어 있습니다.

AD DS 부하를 지 원하는 데 필요한 CPU 수량을 분석 하 고 계산 하는 동안 물리적 하드웨어가 반드시 제대로 매핑되지는 않을 수 있는 측면에서 구입할 수 있는 것에 매핑되는 Cpu 수가 완전히 매핑되지는 않습니다. 가상화를 통해 반올림 하지 않아도 됩니다. 가상화를 사용 하면 VM에 CPU를 추가할 수 있는 작업을 제공 하 여 사이트에 계산 용량을 추가 하는 데 필요한 노력이 줄어듭니다. 게스트에 추가 Cpu를 추가 해야 하는 경우 기본 하드웨어를 사용할 수 있도록 필요한 계산 능력을 정확 하 게 평가할 필요가 없습니다.  항상으로 수요 증가를 계획 하 고 모니터링 해야 합니다.

### <a name="calculation-summary-example"></a>계산 요약 예

|시스템|최대 CPU|
|-|-|-|
|DC 1|120%|
|DC 2|147%|
|Dc 3|218%|
|사용 중인 총 CPU|485%|

|대상 시스템 수|총 대역폭 (위에서)|
|-|-|
|40% 목표에 필요한 Cpu|4.85 &divide; . 4 = 12.25|

이 지점의 중요도에 따라 반복이 증가 하도록 *계획*해야 합니다. 다음 3 년간 50%의 증가율을 가정 하 고이 환경에 &times; 는 3 년 표시에서 18.375 cpu (12.25 1.5)가 필요 합니다. 대체 계획은 첫 해를 검토 한 후 필요에 따라 추가 용량을 추가 하는 것입니다.

### <a name="cross-trust-client-authentication-load-for-ntlm"></a>NTLM을 위한 상호 트러스트 클라이언트 인증 로드

#### <a name="evaluating-cross-trust-client-authentication-load"></a>상호 트러스트 클라이언트 인증 부하 평가

많은 환경에 트러스트에 의해 연결 된 도메인이 하나 이상 있을 수 있습니다. Kerberos 인증을 사용 하지 않는 다른 도메인의 id에 대 한 인증 요청은 도메인 컨트롤러의 보안 채널을 사용 하는 트러스트를 대상 도메인에 있는 다른 도메인 컨트롤러 또는 대상 도메인의 경로에 있는 다음 도메인으로 트래버스 해야 합니다. 도메인 컨트롤러에서 트러스트 된 도메인의 도메인 컨트롤러에 대해 수행할 수 있는 보안 채널을 사용 하는 동시 호출 수는 **MaxConcurrentAPI**이라고 하는 설정에 의해 제어 됩니다. 도메인 컨트롤러의 경우 보안 채널이 부하를 처리할 수 있는지 확인 하는 방법은 튜닝 **MaxConcurrentAPI** 또는 포리스트 내에서 바로 가기 트러스트 만들기의 두 가지 방법 중 하나를 통해 수행 됩니다. 개별 트러스트를 통한 트래픽 양을 측정 하려면 [MaxConcurrentApi 설정을 사용 하 여 NTLM 인증에 대 한 성능 조정을 수행 하는 방법](https://support.microsoft.com/kb/2688798)을 참조 하세요.

데이터를 수집 하는 동안이는 다른 모든 시나리오와 마찬가지로 데이터를 유용 하 게 사용할 수 있는 하루 중 사용량이 많은 기간 중에 수집 되어야 합니다.

> [!NOTE]
> Intraforest 및 인터포리스트 시나리오는 인증이 여러 트러스트를 트래버스할 수 있으며 각 단계를 조정 해야 합니다.

#### <a name="planning"></a>계획

기본적으로 NTLM 인증을 사용 하거나 특정 구성 시나리오에서 사용 하는 여러 응용 프로그램이 있습니다. 응용 프로그램 서버는 용량을 늘리고 서비스는 활성 클라이언트의 수를 증가 시킵니다. 또한 클라이언트는 제한 된 시간 동안 세션을 열어 두고 정기적으로 (예: 전자 메일 끌어오기 동기화) 다시 연결 하는 추세를 가집니다. NTLM 부하가 높은 또 다른 일반적인 예는 인터넷 액세스를 위한 인증이 필요한 웹 프록시 서버입니다.

이러한 응용 프로그램은 NTLM 인증에 상당한 부하를 일으킬 수 있으며, 특히 사용자와 리소스가 서로 다른 도메인에 있는 경우 Dc에 상당한 스트레스를 적용할 수 있습니다.

상호 트러스트 된 부하를 관리 하는 방법에는 여러 가지가 있습니다. 실제로는이를 배타적으로 사용 하거나 시나리오에서 사용 하는 것이 아니라 함께 사용 됩니다. 가능한 옵션은 아래와 같습니다.

- 사용자가 상주 하는 도메인에서 사용자가 사용 하는 서비스를 찾아 트러스트 간 클라이언트 인증을 줄입니다.
- 사용 가능한 보안 채널 수를 늘립니다. 이는 intraforest 및 크로스 포리스트 트래픽과 관련이 있으며 바로 가기 트러스트 라고 합니다.
- **MaxConcurrentAPI**에 대 한 기본 설정을 조정 합니다.

기존 서버에서 튜닝 **MaxConcurrentAPI** 의 경우 수식은 다음과 같습니다.

> *New_MaxConcurrentApi_setting* &ge; (*semaphore_acquires*  +  *semaphore_time*) &times; *average_semaphore_hold_time* &divide; *time_collection_length*

자세한 내용은 [기술 자료 문서 2688798: MaxConcurrentApi 설정을 사용 하 여 NTLM 인증에 대 한 성능 조정을 수행 하는 방법](https://support.microsoft.com/kb/2688798)을 참조 하세요.

## <a name="virtualization-considerations"></a>가상화 고려 사항

없음, 운영 체제 조정 설정입니다.

### <a name="calculation-summary-example"></a>계산 요약 예

|데이터 형식|값|
|-|-|
|세마포 획득 (최소)|6161|
|세마포 획득 (최대)|6762|
|세마포 시간 제한|0|
|평균 세마포 보류 시간|0.012|
|수집 기간 (초)|1:11 분 (71 초)|
|수식 (KB 2688798)|((6762 &ndash; 6161) + 0) &times; 0.012/|
|**MaxConcurrentAPI** 의 최소값|((6762 &ndash; 6161) + 0) &times; 0.012 &divide; 71 =. 101|

이 기간 동안이 시스템의 경우 기본값을 사용할 수 있습니다.

## <a name="monitoring-for-compliance-with-capacity-planning-goals"></a>용량 계획 목표 준수 모니터링

이 문서 전체에서는 계획 및 확장을 사용률 목표 대비로 설명 했습니다. 시스템이 적절 한 용량 임계값 내에서 작동 하는지 확인 하기 위해 모니터링 해야 하는 권장 임계값의 요약 차트는 다음과 같습니다. 이는 성능 임계값이 아니라 용량 계획 임계값 이라는 점에 유의 하세요. 이러한 임계값을 초과 하 여 작동 하는 서버는 작동 하지만 모든 응용 프로그램이 제대로 작동 하는지 확인 하는 것은 시작 시간입니다. 응용 프로그램이 정상적으로 작동 하는 경우 하드웨어 업그레이드 또는 기타 구성 변경 사항을 평가 하기 시작 합니다.

|Category|성능 카운터|간격/샘플링|Target|경고|
|-|-|-|-|-|
|프로세서|프로세서 정보 (_Total) \\ % 프로세서 유틸리티|60분|40%|60%|
|RAM (Windows Server 2008 R2 또는 이전 버전)|Memory\Available MB|< 100|해당 없음|< 100|
|RAM (Windows Server 2012)|트랜잭션 평균 대기 캐시 수명 (초)|30분|테스트 해야 함|테스트 해야 함|
|네트워크|네트워크 인터페이스 ( \* ) \ 보낸 바이트/초<p>네트워크 인터페이스 ( \* ) \ 받은 바이트/초|30분|40%|60%|
|스토리지|논리 디스크 ( *\<NTDS Database Drive\>* ) \Avg Disk sec/Read<p>논리 디스크 ( *\<NTDS Database Drive\>* ) \Avg Disk sec/Write|60분|10ms|15ms|
|AD 서비스|Netlogon ( \* ) \ 평균 세마포 보류 시간|60분|0|1초|

## <a name="appendix-a-cpu-sizing-criteria"></a>부록 A: CPU 크기 조정 기준

### <a name="definitions"></a>정의

**프로세서 (마이크로프로세서) –** 프로그램 명령을 읽고 실행 하는 구성 요소

**CPU-** 중앙 처리 단위

**다중 코어 프로세서 –** 동일한 통합 회로의 여러 cpu

**다중 cpu –** 동일한 통합 회로가 아닌 여러 cpu

**논리적 프로세서-** 운영 체제의 관점에서 논리적 컴퓨팅 엔진 하나

여기에는 다중 코어 프로세서 또는 단일 코어 프로세서에서 하이퍼 스레드, 코어 하나를 포함 합니다.

오늘날의 서버 시스템에는 여러 프로세서, 다중 코어 프로세서 및 하이퍼 스레딩을 포함 하므로이 정보는 두 시나리오를 모두 포함 하 여 일반화 됩니다. 따라서 논리 프로세서 라는 용어는 사용 가능한 컴퓨팅 엔진의 운영 체제 및 응용 프로그램 관점을 나타내는 데 사용 됩니다.

### <a name="thread-level-parallelism"></a>스레드 수준 병렬 처리

각 스레드에는 고유한 스택과 지침이 있으므로 각 스레드는 독립적인 작업입니다. AD DS 다중 스레드 이며 사용 가능한 스레드 수를 [Ntdsutil.exe사용 하 여 Active Directory에서 LDAP 정책을 확인 하 고 설정 하는 방법을 ](https://support.microsoft.com/kb/315071)사용 하 여 튜닝할 수 있으므로 여러 논리 프로세서에서 효과적으로 크기를 조정 합니다.

### <a name="data-level-parallelism"></a>데이터 수준 병렬 처리

이 작업에는 한 프로세스 내에서 여러 스레드 간에 데이터를 공유 하는 작업 (AD DS 프로세스만의 경우) 및 여러 프로세스 (일반적으로)의 여러 스레드 간에 데이터를 공유 하는 작업이 포함 됩니다. 사례를 과도 하 게 간소화 하는 것이 중요 합니다. 즉, 데이터에 대 한 모든 변경 내용이 공유 메모리를 업데이트 하는 것 뿐만 아니라 해당 스레드를 실행 하는 모든 코어에서 모든 다양 한 캐시 수준 (L1, L2, L3)의 모든 실행 중인 스레드에 반영 됩니다. 명령 처리를 계속 하기 전에 모든 다양 한 메모리 위치를 일관 되 게 유지 하면서 쓰기 작업 동안 성능이 저하 될 수 있습니다.

### <a name="cpu-speed-vs-multiple-core-considerations"></a>CPU 속도 및 다중 코어 고려 사항

일반적으로 더 빠른 논리 프로세서를 사용 하면 일련의 명령을 처리 하는 데 걸리는 시간을 줄일 수 있으며, 더 많은 논리 프로세서는 동시에 더 많은 작업을 실행할 수 있음을 의미 합니다. 이러한 역할은 공유 메모리에서 데이터를 가져오고, 데이터 수준 병렬 처리를 대기 하 고, 여러 스레드를 관리 하는 오버 헤드에 대 한 고려 사항으로 인해 중단 됩니다. 이는 다중 코어 시스템의 확장성이 선형이 아닌 이유 이기도 합니다.

이러한 고려 사항에 대 한 고려 사항에는 각 스레드가 개별 자동차이 고 각 레인은 코어 hubs 속도 제한은 클록 속도와 같은 고려 사항에 유의 하세요.

1. 고속도로에 자동차가 하나만 있는 경우 두 레인 또는 12 레인이 있는지 여부는 중요 하지 않습니다. 이 자동차는 속도 제한에서 허용 하는 것 만큼 빠르게 진행 됩니다.
1. 스레드에 필요한 데이터를 즉시 사용할 수 없는 것으로 가정 합니다. 이에 대 한 비유는도로 세그먼트를 종료 하는 것입니다. 고속도로에 자동차가 하나만 있는 경우 레인을 다시 열 때까지 속도 제한이 어느 정도 인지는 중요 하지 않습니다 (데이터를 메모리에서 인출).
1. 자동차 수가 늘어나면 자동차 수를 관리 하는 오버 헤드가 증가 합니다. 주행 시간 (예: 야근 시간)과 트래픽이 많은 경우 (예: 야근 시간은 제외)에는 주행 경험 및 주의가 필요한 지를 비교 합니다. 또한 두 레인 고속도로에서 구동 하는 경우 필요한 주의가 필요한 정도를 고려해 야 합니다. 여기에는 드라이버가 수행 하는 작업에 대해 걱정 하는 다른 레인이 하나 뿐 이며, 그 중 하나는 다른 많은 드라이버가 수행 하는 작업에 대해 걱정 해야 하는 6 차선 고속도로입니다.
   > [!NOTE]
   > 러시 시간 시나리오에 대 한 비유는 다음 섹션에서 설명 합니다. 응답 시간/시스템 Bus을 통해 성능에 영향을 주는 방법

결과적으로 더 많은 프로세서 또는 더 빠른 프로세서에 대 한 세부 정보는 응용 프로그램 동작에 대 한 주관적인이 됩니다 .이 경우 AD DS는 매우 environmentally 특정 환경 내에서 서버에 이르기까지 다양 합니다. 이로 인해 아티클의 앞부분에 있는 참조가 너무 정확 하 게 정확 하 게 투자할 필요가 없으며 계산에 안전 여백이 포함 됩니다. 예산 기반 구매 결정을 내릴 때 더 빠른 프로세서 구매를 고려 하기 전에 먼저 40% (또는 원하는 환경에서 원하는 수)의 프로세서 사용을 최적화 하는 것이 좋습니다. 더 많은 프로세서에서 동기화를 늘리면 선형 진행에서 더 많은 프로세서의 진정한 이점을 줄일 수 &times; 있습니다. 2 프로세서 수는 2 개 미만의 &times; 추가 계산 능력을 제공 합니다.

> [!NOTE]
> Amdahl의 법률 및 Gustafson의 법률은 여기에 관련 된 개념입니다.

### <a name="response-timehow-the-system-busyness-impacts-performance"></a>응답 시간/시스템의 성능에 영향을 주는 방법

큐 이론은 대기 줄 (큐)에 대 한 수학적 연구입니다. 큐 이론에서 사용 법률은 방정식으로 표현 됩니다.

*U* k = *B* &divide; *T*

여기서 *U* k는 사용률 백분율, *B* 는 사용 중인 시간, *T* 는 시스템이 관찰 된 총 시간입니다. Windows 컨텍스트로 변환 됩니다. 즉, 실행 중인 상태에 있는 100 나노초 (ns) 간격 스레드의 수를 지정 된 시간 간격으로 사용할 수 있는 100-ns 간격 수로 나눈 값을 의미 합니다. 이는 정확히% 프로세서 유틸리티 (참조 [프로세서 개체](https://docs.microsoft.com/previous-versions/ms804036(v=msdn.10)) 및 [PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/previous-versions/windows/embedded/ms901169(v=msdn.10)))를 계산 하는 수식입니다.

또한 큐 이론에서는 *n*  =  *u* k &divide; (1 &ndash; *u* k) 수식을 제공 하 여 사용률을 기준으로 대기 중인 항목 수를 예측 합니다. *n* 은 큐의 길이입니다. 모든 사용률 간격에 대해이 차트를 작성 하면 프로세서에서 수신 하는 큐가 지정 된 CPU 로드에 얼마나 오래 걸릴 수 있습니다.

![큐 길이](media/capacity-planning-considerations-queue-length.png)

50%의 CPU 로드 이후 평균에는 항상 큐의 다른 항목 하나가 대기 하 고 약 70%의 CPU 사용률이 증가 하 여 현저 하 게 증가 하는 것을 발견 했습니다.

이 섹션 앞부분에서 사용 된 주행 비유로 돌아갑니다.

- "가상으로"의 바쁜 시간은 40% ~ 70% 범위에 속합니다. 모든 레인을 선택 하는 한 가지 기능을 majorly 제한 하지 않고 다른 드라이버를 사용할 수 있는 기회가 있을 수 있습니다. 반면에 다른 드라이버는 이동 중에 다른 자동차 간의 안전 격차를 "찾을" 필요가 없습니다.
- 그 중 하나는 트래픽이 러시 시간에 도달 하 고,도로 시스템은 100% 용량에 도달 하는 것을 알게 됩니다. 자동차가 변경 될 수 있으므로이 작업을 수행 하는 데 주의 해야 하는 경우에는이 작업을 수행 해야 합니다.

이로 인해 약 40%에 있는 용량의 장기적 평균을 사용 하면 부하가 비정상적으로 급증 하는 경우 (예: 몇 분 동안 실행 되는 잘못 된 코딩 된 쿼리) 또는 일반 부하의 비정상 현상 (예: 긴 주말 이후 첫 번째 날의 아침)에 대해 헤드가 급증 합니다.

위의 문은 일반적인 판독기의 편의를 위해 사용 법률에서 사용 하는 것과 동일한 것 을% Processor Time 계산과 동일 하 게 간주 합니다. 보다 수학적으로 엄격한:
- [PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/previous-versions/windows/embedded/ms901169(v=msdn.10)) 변환
  - *B* = 100-ns 간격 "유휴" 스레드가 논리적 프로세서에서 소비한 시간입니다. [PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/previous-versions/windows/embedded/ms901169(v=msdn.10)) 계산에서 "*X*" 변수의 변경 내용
  - *T* = 지정 된 시간 범위에서 100-ns 간격의 총 수입니다. [PERF_100NSEC_TIMER_INV](https://docs.microsoft.com/previous-versions/windows/embedded/ms901169(v=msdn.10)) 계산에서 "*Y*" 변수의 변경 내용입니다.
  - *U* k = "유휴 스레드" 또는% 유휴 시간에의 한 논리 프로세서의 사용률입니다.
- 수학 계산:
  - *U* k = 1-% 프로세서 시간
  - % Processor Time = 1 – *U* k
  - % Processor Time = 1 – *B*  /  *T*
  - % Processor Time = 1- *X1* – *X0*  /  *Y1* – *Y0*

### <a name="applying-the-concepts-to-capacity-planning"></a>용량 계획에 개념 적용

앞의 수치는 시스템에 필요한 논리적 프로세서 수를 결정 하는 것이 대다수 복잡 한 것 처럼 보일 수 있습니다. 따라서 시스템 크기를 조정 하는 방법은 현재 부하를 기준으로 최대 대상 사용률을 결정 하 고이에 도달 하는 데 필요한 논리적 프로세서 수를 계산 하는 데 중점을 두었습니다. 또한 논리적 프로세서 속도는 성능, 캐시 효율성, 메모리 일관성 요구 사항, 스레드 예약 및 동기화, 그리고 성능이 완벽 하 게 분산 된 클라이언트 로드에 상당한 영향을 주므로 서버 별로 달라질 수 있습니다. 계산 능력을 상대적으로 저렴 한 비용으로, 필요한 Cpu 수를 분석 하 고 결정 하는 것이 비즈니스 가치를 제공 하는 것 보다 학습 연습이 됩니다.

40%는 하드 및 빠른 요구 사항이 아니므로 적절 한 시작입니다. Active Directory의 다양 한 소비자에 게는 다양 한 수준의 응답성이 필요 합니다. 프로세서에 대 한 액세스에 대 한 대기 시간이 증가 하는 경우 클라이언트 성능에 크게 영향을 주지 않으므로 환경이 80% 또는 90% 사용률에서 지속적인 평균으로 실행 될 수 있는 시나리오가 있을 수 있습니다. 시스템에 RAM에 대 한 액세스, 디스크에 대 한 액세스, 네트워크를 통해 응답을 전송 하는 등 시스템의 논리 프로세서 보다 훨씬 느린 여러 영역이 있는지 다시 반복 하는 것이 중요 합니다. 이러한 모든 항목을 함께 조정 해야 합니다. 예제:

- 디스크 바인딩된 90%를 실행 하는 시스템에 더 많은 프로세서를 추가 하면 성능이 크게 향상 되지 않을 수 있습니다. 시스템에 대 한 심층 분석은 i/o가 완료 될 때까지 대기 하는 중 이므로 프로세서에서 발생 하지 않는 많은 스레드가 있음을 식별할 수 있습니다.
- 디스크 바인딩된 문제를 해결 하는 것은 이전에 대기 상태에서 많은 시간을 소비 하는 스레드가 더 이상 i/o의 대기 상태에 있지 않으며, CPU 시간에 대 한 경쟁이 더 많을 것입니다. 즉, 이전 예제의 90% 사용률은 더 이상 사용할 수 없기 때문에 100%로 이동 합니다. 두 구성 요소를 함께 조정 해야 합니다.
  > [!NOTE]
  > 프로세서 정보 (*) \\ % 프로세서 유틸리티는 "터보" 모드를 사용 하는 시스템으로 100%를 초과할 수 있습니다.  이는 CPU가 짧은 기간 동안 평가 된 프로세서 속도를 초과 하는 경우입니다.  자세한 정보를 확인할 때 CPU 제조업체 설명서 및 카운터에 대 한 설명을 참조 하세요.

전체 시스템 사용률 고려 사항을 논의 하면 가상화 된 게스트로 대화 도메인 컨트롤러에도 제공 됩니다. [응답 시간/시스템에서 성능에 미치는 영향은](#response-timehow-the-system-busyness-impacts-performance) 가상화 된 시나리오에서 호스트와 게스트 모두에 적용 됩니다. 이는 하나의 게스트만 있는 호스트에서 도메인 컨트롤러 (및 일반적으로 모든 시스템)가 실제 하드웨어에서 수행 하는 것과 거의 동일 합니다. 호스트에 추가 게스트를 추가 하면 기본 호스트의 사용률이 높아지므로 이전에 설명한 대로 프로세서에 대 한 액세스 권한을 얻기 위해 대기 시간이 늘어납니다. 즉, 논리 프로세서 사용률은 호스트와 게스트 수준 모두에서 관리 해야 합니다.

고속도로를 물리적 하드웨어로 두고 이전 hubs를 확장 하면 게스트 VM이 버스 (rider에서 원하는 대상으로 바로 이동 하는 express 버스)와 analogized 됩니다. 다음 네 가지 시나리오를 상상해 보세요.

- 시간이 꺼져 있고, 거의 비어 있는 버스에서 rider을 가져오고, 버스가 거의 비어 있는 이동에서 가져옵니다. 경쟁 하는 트래픽이 없기 때문에 rider는 매우 쉽게 짐작할 수 있으며 rider가 대신 구동 되는 것 처럼 빠르게 진행 됩니다. Rider의 이동 시간은 여전히 속도 제한에 의해 제한 됩니다.
- 이는 시간이 꺼져 있으므로 버스가 거의 비어 있지만 이동 중에 대부분의 레인은 닫혀 있으므로 고속도로 계속 정체 됩니다. Rider은 혼잡 한도로 거의 비어 있는 버스에 있습니다. Rider에는 버스에 대 한 많은 경쟁이 있지만,이 경우에는 외부 트래픽의 나머지 부분에도 전체 이동 시간이 결정 됩니다.
- 고속도로 및 bus가 정체 되기 때문에이는 러시 시간입니다. 사용자가 어깨를 활용 하는 것이 더 긴 하지만, 사용자가 어깨를 활용 하는 것이 훨씬 더 고속도로 때문에 버스를 설정 및 해제 하는 것은 매우 다양 합니다. 버스 (논리 프로세서)를 추가 하는 것은 더 쉽게 이동 하거나 이동이 줄어들 수 있는 것이 아닙니다.
- 최종 시나리오는이에 대 한 간단한 문제를 해결할 수 있지만, 버스는 전체 이지만 이동은 혼잡 하지 않습니다. Rider는 계속 해 서 버스를 설정 및 해제 하는 데 문제가 있지만 이동이 진행 되는 동안에는 이동이 효율적으로 진행 됩니다. 이 시나리오는 버스 (논리 프로세서를 게스트에 추가)를 추가 하 여 게스트 성능을 향상 시키는 유일한 시나리오입니다.

여기에서 사용 하는 경우에는 0%가 사용 되는 것과 이동의 100% 활용 상태와 다양 한 수준의 영향이 있는 버스의 0% 및 100% 사용 상태 사이에 많은 시나리오가 있다는 것을 비교적 쉽게 예측할 수 있습니다.

40% CPU 이상의 보안 주체를 호스트의 적절 한 목표와 게스트에 적용 하는 것은 위의 큐 양과 동일한 추론을 위해 적절 한 시작입니다.

## <a name="appendix-b-considerations-regarding-different-processor-speeds-and-the-effect-of-processor-power-management-on-processor-speeds"></a>부록 B: 여러 프로세서 속도와 관련 된 고려 사항 및 프로세서 속도에 대 한 프로세서 전원 관리의 영향

프로세서 선택에 대 한 섹션 전체에서 프로세서가 데이터를 수집 하는 전체 시간에 대 한 100%의 클록 속도와 교체 시스템의 속도 프로세서가 동일한 것으로 가정 합니다. 특히 Windows Server 2008 R2 이상에서 기본 전원 계획의 **균형**을 유지 하는 것 처럼 가정 하는 것이 모두 false 인 경우에도 방법론은 여전히 보수적인 방법 이라는 것을 의미 합니다. 잠재적인 오류 비율이 증가 하는 경우 프로세서 속도가 증가 함에 따라 보안의 비율도 증가 합니다.

- 예를 들어 11.25 Cpu가 요구 되는 시나리오에서 데이터가 수집 될 때 프로세서가 절반 속도로 실행 되는 경우 더 정확한 예상 값은 5.125 2 일 수 있습니다 &divide; .
- 클록 속도를 두 배로 늘리면 지정 된 기간 동안 발생 하는 처리의 양이 두 배가 되도록 보장할 수 없습니다. 이는 프로세서에서 RAM 또는 다른 시스템 구성 요소를 대기 하는 데 걸리는 시간이 동일 하 게 유지 될 수 있기 때문입니다. 결과적으로, 데이터를 인출 하기 위해 대기 하는 동안 더 빠른 프로세서가 유휴 시간에 더 많은 백분율을 소비할 수 있습니다. 다시, 프로세서 속도 간의 선형 비교를 가정 하 여 가장 낮은 공통 분모를 유지 하 고, 신중 하 게 유지 하 고, 잠재적으로 거짓 수준의 정확도를 계산 하는 것을 방지 하는 것이 좋습니다.

또는 교체 하드웨어의 프로세서 속도가 현재 하드웨어 보다 낮은 경우에는 필요한 프로세서의 예상 크기를 비례적으로 늘리는 것이 안전 합니다. 예를 들어 사이트의 부하를 유지 하기 위해 10 개의 프로세서가 필요 하 고 현재 프로세서는 3.3 g h z에서 실행 되며 교체 프로세서는 2.6 g h z에서 실행 되며, 속도는 21% 감소 합니다. 이 경우 12 개의 프로세서가 권장 금액이 됩니다.

즉, 이러한 산포도는 용량 관리 프로세서 사용률 목표를 변경 하지 않습니다. 프로세서 클록 속도가 요청 된 부하에 따라 동적으로 조정 되므로, 높은 부하 상태에서 시스템을 실행 하면 CPU가 더 높은 클록 속도 상태에서 더 많은 시간을 소비 하 여 최대 100% 클록 속도 상태에서 최종 목표가 40%의 사용률을 갖는 시나리오가 생성 됩니다. CPU 속도는 사용량이 많은 시나리오에서 발생 하는 시간을 초과 하므로 전력 절감 액을 생성 하는 것 보다 더 낮습니다.

> [!NOTE]
> 데이터를 수집 하는 동안 프로세서에서 전원 관리를 해제 하 여 전원 계획을 **고성능**으로 설정 하는 옵션을 사용할 수 있습니다. 그러면 대상 서버에서 CPU 소비를 보다 정확 하 게 표현할 수 있습니다.

다른 프로세서에 대 한 예상 값을 조정 하기 위해, 위에 설명 된 다른 시스템 병목 상태를 제외 하 고는 안전 하 게 사용 되어 두 번째 프로세서의 속도를 수행할 수 있는 처리량이 두 배로 증가 한다고 가정 합니다.  현재 프로세서의 내부 아키텍처는 프로세서 간에 충분 하며, 데이터와 다른 프로세서를 사용 하는 경우의 효과를 측정 하는 보다 안전한 방법은 표준 Performance Evaluation Corporation의 SPECint_rate2006 벤치 마크를 활용 하는 것입니다.

1. 사용 중인 프로세서의 SPECint_rate2006 점수와 사용할 계획을 찾습니다.
    1. Standard Performance Evaluation Corporation의 웹 사이트에서 **결과**를 선택 하 고 **CPU2006**를 강조 표시 한 다음 **모든 SPECint_rate2006 결과 검색**을 선택 합니다.
    1. **단순 요청**에서 대상 프로세서에 대 한 검색 조건을 입력 합니다. 예를 들어 **Processor와 일치 하는 2630 (Baselinetarget)** 및 **프로세서가 e5-2650 (기준선)** 과 일치 합니다.
    1. 사용할 서버 및 프로세서 구성 (또는 정확히 일치 하는 항목을 사용할 수 없는 경우 닫기)을 찾아 **결과** 및 **# 코어** 열의 값을 확인 합니다.
1. 한정자를 결정 하려면 다음 수식을 사용 합니다.
   >((*대상 플랫폼/코어 점수 값*) &times; (*기본 플랫폼의 코어 당 MHz*) ( &divide; (*기준 코어 점수 값 기준*) ( &times; *대상 플랫폼의 코어 당 MHz*)

    위의 예제를 사용 합니다.
   >(35.83 &times; 2000) &divide; (33.75 &times; 2300) = 0.92
1. 예상 프로세서 수를 한정자와 곱합니다.  위의 경우에 E5-2650 프로세서에서 E5-2630 프로세서로 이동 하려면 필요한 계산 된 11.25 Cpu &times; 0.92 = 10.35 프로세서를 곱합니다.

## <a name="appendix-c-fundamentals-regarding-the-operating-system-interacting-with-storage"></a>부록 C: 저장소와 상호 작용 하는 운영 체제에 대 한 기본 사항

[응답 시간/시스템 bus를 통한 성능 영향 방법](#response-timehow-the-system-busyness-impacts-performance) 에 설명 된 큐 이론 개념은 저장소에도 적용 됩니다. 이러한 개념을 적용 하기 위해 운영 체제에서 i/o를 처리 하는 방법을 잘 알고 있어야 합니다. Microsoft Windows 운영 체제에서 각 실제 디스크에 대 한 i/o 요청을 보유 하는 큐가 만들어집니다. 그러나 실제 디스크에 대 한 설명이 필요 합니다. 배열 컨트롤러와 San은 단일 실제 디스크로 운영 체제에 스핀 들의 집계를 제공 합니다. 또한 배열 컨트롤러와 San은 여러 디스크를 하나의 배열 집합으로 집계 한 다음이 배열 집합을 여러 개의 "파티션"으로 분할할 수 있으며,이는 운영 체제에 여러 물리적 디스크로 표시 됩니다 (참조).

![블록 스핀 들](media/capacity-planning-considerations-block-spindles.png)

이 그림에서는 두 개의 스핀 들이 미러된 후 데이터 저장소 (데이터 1 및 데이터 2)의 논리적 영역으로 분할 됩니다. 이러한 논리적 영역은 별도의 실제 디스크로 운영 체제에서 볼 수 있습니다.

매우 복잡할 수 있지만이 부록 전체에서 다음과 같은 용어를 사용 하 여 다양 한 엔터티를 식별 합니다.

- **스핀 들 –** 서버에 물리적으로 설치 된 장치입니다.
- **Array –** 컨트롤러에서 집계 하는 스핀 들의 컬렉션입니다.
- **배열 파티션 –** 집계 된 배열의 분할
- **LUN –** san을 참조할 때 사용 되는 배열입니다.
- **디스크 –** 운영 체제에서 단일 실제 디스크를 관찰 하는 것을 말합니다.
- **파티션 –** 운영 체제가 실제 디스크로 인식 하는 논리적 분할입니다.

### <a name="operating-system-architecture-considerations"></a>운영 체제 아키텍처 고려 사항

운영 체제는 관찰 되는 각 디스크에 대해 FIFO (선입 선출) i/o 큐를 만듭니다. 이 디스크는 스핀 들, 배열 또는 배열 파티션을 나타낼 수 있습니다. 운영 체제 관점에서 i/o 처리와 관련 하 여 더 많은 활성 큐를 사용 하는 것이 더 좋습니다. FIFO 큐는 serialize 되므로 저장소 하위 시스템에 대해 발급 된 모든 i/o는 요청이 도착 한 순서 대로 처리 되어야 합니다. 운영 체제에서 관찰 되는 각 디스크를 스핀 들/배열에 상관 관계를 설정 하 여, 운영 체제는 각 고유 디스크 집합에 대 한 i/o 큐를 유지 관리 하므로 디스크에서 부족 한 i/o 리소스에 대 한 경합이 제거 되 고 i/o 요구가 단일 디스크로 격리 됩니다. 단, Windows Server 2008에서는 i/o 우선 순위 지정 이라는 개념을 소개 하 고 "낮은" 우선 순위를 사용 하도록 설계 된 응용 프로그램은이 일반적인 주문에서 제외 되 고 뒤로 이동 합니다. "낮음" 우선 순위 기본값을 활용 하기 위해 특별히 코딩 되지 않은 응용 프로그램은 "보통"입니다.

### <a name="introducing-simple-storage-subsystems"></a>간단한 저장소 하위 시스템 소개

간단한 예제 (컴퓨터 내의 단일 하드 드라이브)로 시작 하 여 구성 요소에 대 한 분석이 제공 됩니다. 이를 주요 저장소 하위 시스템 구성 요소로 분리 하면 시스템은 다음으로 구성 됩니다.

- **1 –** 1만 RPM ULTRA FAST scsi HD (ULTRA fast scsi의 전송 속도는 20mb 임)
- **1 –** SCSI 버스 (케이블)
- **1 –** Ultra Fast SCSI 어댑터
- **1 –** 32 비트 33 MHz PCI 버스

구성 요소가 식별 되 면 시스템을 전송할 수 있는 데이터의 양 또는 처리할 수 있는 i/o의 양에 대 한 아이디어를 계산할 수 있습니다. 시스템을 전송할 수 있는 i/o와 데이터의 양은 상관 관계가 있지만 동일 하지는 않습니다. 이러한 상관 관계는 디스크 i/o가 무작위로 또는 순차적이 고 블록 크기 인지에 따라 달라 집니다. 모든 데이터는 블록으로 디스크에 기록 되지만 다른 블록 크기를 사용 하는 응용 프로그램은 서로 다릅니다. 구성 요소별 기준:

- **하드 드라이브-** 평균 1만-RPM 하드 드라이브에는 7 밀리초 (밀리초) 검색 시간 및 3 밀리초 액세스 시간이 있습니다. Seek time은 읽기/쓰기 헤드를 기준 위치로 이동 하는 데 걸리는 평균 시간입니다. 액세스 시간은 데이터가 올바른 위치에 있는 경우 디스크에 데이터를 읽거나 쓰는 데 걸리는 평균 시간입니다. 따라서 1만-RPM HD에서 고유 데이터 블록을 읽는 평균 시간은 데이터 블록 당 총 약 10 밀리초 (또는 .010 초)의 검색 및 액세스를 구성 합니다.

  모든 디스크 액세스에서 디스크의 새 위치로 헤드를 이동 해야 하는 경우 읽기/쓰기 동작을 "임의" 라고 합니다. 따라서 모든 i/o가 임의로 인 경우 1만 RPM HD는 약 100 IOPS (초당 i/o)를 처리할 수 있습니다. 수식은 초당 1000 밀리초를 i/o 당 10 밀리초 또는 1000/10 = 100 IOPS로 나눈 값입니다.

  또는 모든 i/o가 HD의 인접 섹터에서 발생 하는 경우이를 순차적 i/o 라고 합니다. 첫 번째 i/o가 완료 되 면 읽기/쓰기 헤드는 다음 데이터 블록이 HD에 저장 되는 시작 부분에 있기 때문에 순차적 i/o에는 검색 시간이 없습니다. 따라서 1만 RPM HD는 초당 약 333 i/o를 처리할 수 있습니다 (초당 초당 1000 밀리초는 i/o 당 3 밀리초로 나눈 값).

  > [!NOTE]
  > 이 예제는 단일 실린더의 데이터가 일반적으로 유지 되는 디스크 캐시를 반영 하지 않습니다. 이 경우 첫 번째 i/o에 10 밀리초가 필요 하 고 디스크에서 전체 실린더를 읽습니다. 다른 모든 순차 i/o가 캐시에서 충족 됩니다. 결과적으로 디스크 내 캐시는 순차적 i/o 성능을 향상 시킬 수 있습니다.

  지금까지 하드 드라이브의 전송 율은 관련이 없습니다. 하드 드라이브가 20mb/s 울트라 또는 Ultra3 160 m b/s 인지 여부에 관계 없이 1만-RPM HD로 처리할 수 있는 실제 IOPS 양은 ~ 100 무작위 또는 ~ 300 순차 i/o입니다. 드라이브에 기록 하는 응용 프로그램에 따라 블록 크기가 변경 되 면 i/o 당 끌어올 수 있는 데이터의 양이 다릅니다. 예를 들어 블록 크기가 8kb 인 경우 100 i/o 작업은 하드 드라이브에서 전체 800 KB를 읽고 씁니다. 그러나 블록 크기가 32 KB 인 경우 100 i/o는 하드 드라이브에 3200 KB (3.2 MB)를 읽고 씁니다. SCSI 전송 속도가 전송 되는 총 데이터 양을 초과 하는 경우 "고속" 전송 속도 드라이브를 가져오면 아무것도 발생 하지 않습니다. 비교는 다음 표를 참조 하세요.

  | |7200 RPM 9ms seek, 4ms 액세스|1만 RPM 7ms seek, 3ms access|15000 RPM 4ms seek, 2ms access
  |-|-|-|-|
  |임의 i/o|80|100|150|
  |순차 i/o|250|300|500|

  |1만 RPM 드라이브|8kb 블록 크기 (Active Directory Jet)|
  |-|-|
  |임의 i/o|800 k b/초|
  |순차 i/o|2400 k b/초|

- **SCSI 후면판 (버스)-** "SCSI 후면판 (버스)" 또는이 시나리오의 리본 케이블을 이해 하는 것은 저장소 하위 시스템의 처리량에 미치는 영향에 따라 달라 집니다. 기본적으로, i/o가 8kb 블록 내에 있는 경우 버스에서 처리할 수 있는 i/o 수는 얼마 인가요? 이 시나리오에서 SCSI 버스는 20mb/s 또는 20480 k b/초입니다. 20480 k b/초를 8kb 블록으로 나누면 SCSI 버스에서 지 원하는 최대 2500 IOPS가 생성 됩니다.

  > [!NOTE]
  > 다음 표의 그림은 예를 나타냅니다. 대부분의 연결 된 저장소 장치는 현재 PCI Express를 사용 하 여 더 높은 처리량을 제공 합니다.

  |블록 크기 당 SCSI 버스에서 지원 되는 i/o|2kb 블록 크기|8kb 블록 크기 (AD Jet) (SQL Server 7.0/SQL Server 2000)
  |-|-|-|
  |20MB/s|10000|2,500|
  |40MB/초|20,000|5,000|
  |128 m b/초|65,536|16,384|
  |320 m b/초|160,000|40,000|

  이 차트에서 확인할 수 있듯이, 사용 되는 항목에 관계 없이, 버스에 병목 현상이 발생 하지는 않습니다. 스핀 들 최대값은 100 i/o이 고 위의 임계값 보다 낮게 표시 되기 때문입니다.

  > [!NOTE]
  > 이는 SCSI 버스가 100% 효율적 이라고 가정 합니다.

- **SCSI 어댑터 –** 이에서 처리할 수 있는 i/o의 양을 결정 하려면 제조업체의 사양을 확인 해야 합니다. 적절 한 장치에 대 한 i/o 요청을 전송 하려면 일종의 처리가 필요 합니다. 따라서 처리할 수 있는 i/o의 양은 SCSI 어댑터 (또는 배열 컨트롤러) 프로세서에 따라 다릅니다.

  이 예제에서는 1000 i/o가 처리 될 수 있다고 가정 합니다.

- **PCI 버스 –** 이는 자주 간과 되는 구성 요소입니다. 이 예에서는 병목 현상이 발생 하지 않습니다. 그러나 시스템이 확장 될 때 병목 상태가 될 수 있습니다. 참조를 위해 33Mhz에서 작동 하는 32 비트 PCI 버스가 이론적으로 133 m b/s의 데이터를 전송할 수 있습니다. 수식은 다음과 같습니다.
  > 32 비트, &divide; 바이트 &times; 33 MHz = 133 m b/초 당 8 비트

  는 이론상의 제한 사항입니다. 실제로는 최대 50%에 도달 하지만 특정 버스트 시나리오에서는 짧은 기간 동안 75%의 효율성을 얻을 수 있습니다.

  66Mhz 64 비트 PCI bus는 이론적 최대값 ( &divide; 바이트 66 Mhz 당 64 비트 8 비트 &times; ) = 528 m b/초를 지원할 수 있습니다. 또한 다른 모든 장치 (예: 네트워크 어댑터, 두 번째 SCSI 컨트롤러 등)는 대역폭을 공유 하 고 장치가 제한 된 리소스에 대해 경합 될 때 사용할 수 있는 대역폭을 줄입니다.

이 저장소 하위 시스템의 구성 요소를 분석 한 후 스핀 들은 요청할 수 있는 i/o의 양과 시스템을 전송할 수 있는 데이터의 양을 제한 하는 요소입니다. 특히 AD DS 시나리오에서 Jet 데이터베이스에 액세스할 때 초당 총 800 k b의 초당 8kb의 초당 i/o 수가 100입니다. 또는 로그 파일에 독점적으로 할당 되는 스핀 들에 대 한 최대 처리량에는 초당 300 초당 순차 i/o가 8kb 단위로 증가 하 여 초당 총 2400 KB (2.4 MB)의 제한이 적용 됩니다.

이제 간단한 구성을 분석 한 후 다음 표에서는 저장소 하위 시스템의 구성 요소가 변경 되거나 추가 될 때 병목 현상이 발생 하는 위치를 보여 줍니다.

|메모|병목 상태 분석|디스크|버스|어댑터|PCI 버스|
|-|-|-|-|-|-|
|이는 두 번째 디스크를 추가한 후의 도메인 컨트롤러 구성입니다. 디스크 구성은 800 k b/초에서 병목 상태를 나타냅니다.|1 개 디스크 추가 (합계 = 2)<p>I/o가 임의적입니다.<p>4kb 블록 크기<p>1만 RPM HD|200 I/Os 합계<br />800 KB/s 합계입니다.| | | |
|7 개의 디스크를 추가한 후 디스크 구성은 여전히 3200 k b/s에서 병목 상태를 나타냅니다.|**7 개 디스크 추가 (합계 = 8)**  <p>I/o가 임의적입니다.<p>4kb 블록 크기<p>1만 RPM HD|800 I/Os 합계입니다.<br />3200 총 k b/초| | | |
|I/o를 순차적으로 변경한 후에는 네트워크 어댑터가 1000 IOPS로 제한 되기 때문에 병목 상태가 됩니다.|7 개 디스크 추가 (합계 = 8)<p>**I/o가 순차적입니다.**<p>4kb 블록 크기<p>1만 RPM HD| | |2400 i/o를 디스크에 읽고 쓸 수 있으며, 컨트롤러는 1000 IOPS로 제한 됩니다.| |
|네트워크 어댑터를 1만 IOPS를 지 원하는 SCSI 어댑터로 바꾸면 병목 현상이 디스크 구성으로 돌아옵니다.|7 개 디스크 추가 (합계 = 8)<p>I/o가 임의적입니다.<p>4kb 블록 크기<p>1만 RPM HD<p>**SCSI 어댑터 업그레이드 (이제 1만 i/o 지원)**|800 I/Os 합계입니다.<br />3200 총 k b/초| | | |
|블록 크기를 32 KB로 늘려도 버스는 20mb/s만 지원 하기 때문에 병목 상태가 됩니다.|7 개 디스크 추가 (합계 = 8)<p>I/o가 임의적입니다.<p>**32 블록 크기**<p>1만 RPM HD| |800 I/Os 합계입니다. 25600 KB/s (25MB/s)를 디스크에 읽고 쓸 수 있습니다.<p>버스는 20mb/s만 지원 합니다.| | |
|버스를 업그레이드 하 고 디스크를 더 추가 하면 디스크는 병목 상태를 유지 합니다.|**13 개의 디스크 추가 (합계 = 14)**<p>14 개의 디스크가 있는 두 번째 SCSI 어댑터 추가<p>I/o가 임의적입니다.<p>4kb 블록 크기<p>1만 RPM HD<p>**320 m b/s SCSI 버스로 업그레이드**|2800 i/o<p>11200 k b/초 (10.9 m b/초)| | | |
|I/o를 순차적으로 변경한 후 디스크는 병목 상태를 유지 합니다.|13 개의 디스크 추가 (합계 = 14)<p>14 개의 디스크가 있는 두 번째 SCSI 어댑터 추가<p>**I/o가 순차적입니다.**<p>4kb 블록 크기<p>1만 RPM HD<p>320 m b/s SCSI 버스로 업그레이드|8400 i/o<p>33600 KB\s<p>(32.8 MB\s)| | | |
|하드 드라이브를 더 빨리 추가한 후에는 디스크에 병목 상태가 남아 있습니다.|13 개의 디스크 추가 (합계 = 14)<p>14 개의 디스크가 있는 두 번째 SCSI 어댑터 추가<p>I/o가 순차적입니다.<p>4kb 블록 크기<p>**15000 RPM HD**<p>320 m b/s SCSI 버스로 업그레이드|14000 i/o<p>56000 k b/초<p>(54.7 m b/초)| | | |
|블록 크기를 32 KB로 늘린 후 PCI 버스가 병목 상태가 됩니다.|13 개의 디스크 추가 (합계 = 14)<p>14 개의 디스크가 있는 두 번째 SCSI 어댑터 추가<p>I/o가 순차적입니다.<p>**32 블록 크기**<p>15000 RPM HD<p>320 m b/s SCSI 버스로 업그레이드| | | |14000 i/o<p>448000 k b/초<p>(437 m b/s)는 스핀 들에 대 한 읽기/쓰기 제한입니다.<p>PCI 버스는 이론적으로 최대 133 m b/초를 지원 합니다 (최대한 75% 효율적).|

### <a name="introducing-raid"></a>RAID 소개

배열 컨트롤러를 도입할 때 저장소 하위 시스템의 특성은 크게 변경 되지 않습니다. 계산에서 SCSI 어댑터를 대체 합니다. 다양 한 배열 수준 (예: RAID 0, RAID 1 또는 RAID 5)을 사용 하는 경우 디스크에 대 한 데이터를 읽고 쓰는 데 드는 비용을 변경 해야 합니다.

RAID 0에서 데이터는 RAID 집합의 모든 디스크에 걸쳐 스트라이프 됩니다. 즉, 읽기 또는 쓰기 작업을 수행 하는 동안 데이터의 일부를 각 디스크에서 끌어올 수 있으며, 동일한 기간에 시스템을 전송할 수 있는 데이터의 양이 늘어납니다. 따라서 1 초 안에 각 스핀 들 (1만 RPM 드라이브 라고 가정)에서 100 i/o 작업을 수행할 수 있습니다. 지원할 수 있는 i/o의 총 크기는 스핀 들 당 초당 N 개의 스핀 들 시간 100 i/o (초당 100 * N i/o)입니다.

![논리적 d: 드라이브](media/capacity-planning-considerations-logical-d-drive.png)

RAID 1에서는 중복성을 위해 한 쌍의 스핀 들에서 데이터가 미러 (복제) 됩니다. 따라서 읽기 i/o 작업을 수행 하는 경우 집합의 두 스핀 들 모두에서 데이터를 읽을 수 있습니다. 이렇게 하면 읽기 작업을 수행 하는 동안 두 디스크의 i/o 용량을 효과적으로 사용할 수 있습니다. 쓰기 작업은 RAID 1에서 성능 이점을 얻을 수 없다는 것입니다. 중복성을 위해 동일한 데이터를 두 드라이브에 기록해 야 하기 때문입니다. 두 스핀 들이 모두 데이터를 복제 하기 때문에 데이터 쓰기가 더 이상 수행 되지 않지만 두 스핀 들이 모두 데이터를 복제 하기 때문에 기본적으로 쓰기 i/o 작업을 수행 하면 두 개의 읽기 작업이 발생 하지 않습니다. 따라서 모든 쓰기 i/o 비용은 두 개의 읽기 i/o입니다. 해당 정보에서 수식을 만들어 발생 하는 i/o 작업의 총 수를 확인할 수 있습니다.

> *읽기* i/o + 2 &times; *쓰기 i/o*  =  *사용 가능한 총 디스크 i/o 수*

쓰기에 대 한 읽기 비율 및 스핀 들 수를 알고 있는 경우 위의 수식에서 다음 수식을 파생 하 여 배열에서 지원할 수 있는 최대 i/o를 식별할 수 있습니다.

> *스핀 들* &times; 당 최대 IOPS 2 개의 스핀 들 &times; [(%*읽기*  +  *% 쓰기*) &divide; (*% 읽기* + 2 &times; *% 쓰기*)] = *총 IOPS*

RAID 1 + 0은 읽기 및 쓰기 비용과 관련 하 여 RAID 1과 정확히 동일 하 게 동작 합니다. 그러나 i/o는 이제 각 미러된 집합에서 스트라이프 됩니다. 조건

> *스핀 들* &times; 당 최대 IOPS 2 개의 스핀 들 &times; [(%*읽기*  +  *% 쓰기*) &divide; (*% 읽기* + 2 &times; *% 쓰기*)] = *총* i/o

RAID 1 집합에서, RAID 1 집합의 복합성 (*n*)이 스트라이프 되 면, 처리할 수 있는 총 I/O가 &times; raid 1 집합 당 n i/o가 됩니다.

> *N* &times; {*스핀 들 당 최대 iops* &times; 2 스핀 들 &times; [(%*reads*  +  *% writes*) &divide; (%*reads* + 2 &times; *% writes*)]} = *총 iops*

RAID 5에서 때때로 *n* + 1 raid 라고도 하는 데이터는 *n* 개의 스핀 들 간에 스트라이프 되며 패리티 정보는 "+ 1" 스핀 들에 기록 됩니다. 그러나 raid 1 또는 1 + 0 보다는 쓰기 i/o를 수행할 때 RAID 5가 훨씬 더 비쌉니다. RAID 5는 쓰기 i/o가 배열에 제출 될 때마다 다음 프로세스를 수행 합니다.

1. 이전 데이터 읽기
1. 이전 패리티 읽기
1. 새 데이터 작성
1. 새 패리티를 작성 합니다.

운영 체제에서 배열 컨트롤러에 제출 하는 모든 쓰기 i/o 요청은 완료 하는 데 네 번의 i/o 작업이 필요 하므로 제출 되는 쓰기 요청은 단일 읽기 i/o로 완료 하는 데 걸리는 시간으로 4 번 소요 됩니다. 운영 체제 관점에서 스핀 들에 의해 발생 한 i/o 요청을 변환 하는 수식을 파생 시키려면 다음을 수행 합니다.

> *읽기* i/o + 4 쓰기 i/o &times; *Write I/O*  =  *총* i/o

RAID 1 집합에서 쓰기에 대 한 읽기 및 스핀 들 수의 비율을 알고 있는 경우 다음 수식을 위의 수식에서 파생 하 여 배열에서 지원할 수 있는 최대 i/o를 식별할 수 있습니다 (총 스핀 들 수에 "드라이브"가 패리티를 포함 하지 않음).

> *스핀 들* &times; 당 IOPS (*스핀 들* – 1) &times; [(%*읽기*  +  *% 쓰기*) &divide; (%*읽기* + 4 &times; *% 쓰기*)] = *총 IOPS*

### <a name="introducing-sans"></a>San 소개

저장소 하위 시스템의 복잡성을 확장 하는 경우 SAN이 환경에 도입 되 면 설명 된 기본 원칙이 변경 되지 않지만 SAN에 연결 된 모든 시스템의 i/o 동작은 고려해 야 합니다. SAN을 사용 하는 주요 이점 중 하나는 내부적으로 또는 외부에서 연결 된 저장소에 대 한 추가적인 중복성입니다. 이제 용량 계획을 통해 내결함성 요구 사항을 고려해 야 합니다. 또한 평가 해야 하는 더 많은 구성 요소가 도입 되었습니다. SAN을 구성 요소 부분으로 나눕니다.

- SCSI 또는 파이버 채널 하드 드라이브
- 저장소 유닛 채널 백플레인
- 저장 단위
- 저장소 컨트롤러 모듈
- SAN 스위치
- HBA (s)
- PCI 버스

중복성을 위해 시스템을 설계 하는 경우 오류 발생 가능성을 수용할 수 있도록 추가 구성 요소가 포함 됩니다. 용량 계획 시 사용 가능한 리소스에서 중복 구성 요소를 제외 하는 것이 매우 중요 합니다. 예를 들어 SAN에 두 개의 컨트롤러 모듈이 있는 경우 하나의 컨트롤러 모듈에 대 한 i/o 용량은 시스템에서 사용할 수 있는 총 i/o 처리량에 사용 해야 합니다. 이는 하나의 컨트롤러에 오류가 발생 하는 경우 나머지 컨트롤러에서 연결 된 모든 시스템에서 요구 하는 전체 i/o 로드를 처리 해야 하기 때문입니다. 최대 사용 기간에 대 한 모든 용량 계획을 수행 하기 때문에 중복 구성 요소를 사용 가능한 리소스로 팩터링 해서는 안 되며, 계획 된 최대 사용률이 시스템의 80% 포화를 초과 하지 않아야 합니다 (버스트 또는 비정상적인 시스템 동작을 수용 하기 위해). 마찬가지로, 중복 SAN 스위치, 저장소 단위 및 스핀 들은 i/o 계산으로 팩터링 되어서는 안 됩니다.

SCSI 또는 파이버 채널 하드 드라이브의 동작을 분석 하는 경우 앞에서 설명한 대로 동작을 분석 하는 방법은 변경 되지 않습니다. 각 프로토콜에 대 한 특정 장점 및 단점이 있지만 디스크당 제한 요소는 하드 드라이브의 기계적 제한 사항입니다.

저장소 단위에서 채널을 분석 하는 것은 SCSI 버스에서 사용할 수 있는 리소스를 계산 하는 것과 정확히 동일 하며, 크기 (예: 20MB/s)를 블록 크기로 나눈 값 (예: 8kb)입니다. 이는 위의 간단한 예제와는 여러 채널의 집계에 있습니다. 예를 들어 각각 20mb/s 최대 전송 비율을 지 원하는 6 개의 채널이 있는 경우 사용할 수 있는 i/o 및 데이터 전송의 총 크기는 100 m b/초입니다 (이는 120 MB/s가 아님). 또한이 계산에서 전체 채널이 손실 될 경우 시스템은 5 개의 작동 하는 채널로만 남아 있습니다. 따라서 오류 발생 시 성능 기대치를 지속적으로 충족 하기 위해 모든 저장소 채널에 대 한 총 처리량은 100 m b/초를 초과 하면 안 됩니다 .이 경우 로드 및 내결함성이 모든 채널에 고르게 분산 된 것으로 가정 합니다. 이를 i/o 프로필로 설정 하는 것은 응용 프로그램의 동작에 따라 달라 집니다. Active Directory Jet i/o의 경우,이는 초당 약 12500 i/o (초당 100 m b/s &divide; 8kb)와 관련이 있습니다.

다음으로, 각 모듈이 지원할 수 있는 처리량을 이해 하기 위해 컨트롤러 모듈에 대 한 제조업체의 사양을 획득 해야 합니다. 이 예에서 SAN에는 각각 7500 i/o를 지 원하는 두 개의 컨트롤러 모듈이 있습니다. 중복성이 필요 하지 않은 경우 시스템의 총 처리량은 15000 IOPS가 될 수 있습니다. 오류가 발생 하는 경우 최대 처리량을 계산할 때 한 컨트롤러 또는 7500 IOPS의 처리량이 제한 됩니다. 이 임계값은 모든 저장소 채널에서 지원할 수 있는 최대 12500 IOPS (4kb 블록 크기 라고 가정) 보다 훨씬 더 작습니다. 따라서 현재 분석에서 병목 현상이 발생 합니다. 계획을 위해 계획을 세울 때에는 원하는 최대 i/o를 10400 i/o로 설정할 수 있습니다.

데이터는 컨트롤러 모듈을 종료 하면 1 g b/s (또는 초당 1 기가 비트)로 평가 되는 파이버 채널 연결을 전송은 합니다. 다른 메트릭과의 상관 관계를 위해 1gb/s는 128 m b/초 (1 g b/초 &divide; 8 비트/바이트)로 전환 됩니다. 이는 저장소 단위 (100 m b/초)의 모든 채널에서 총 대역폭을 초과 하므로 시스템에 병목 현상이 발생 하지 않습니다. 또한 두 채널 중 하나인 경우 (추가 1 g b/s 파이버 채널 중복성에 대 한 연결), 한 연결이 실패 하는 경우 나머지 연결에는 필요한 모든 데이터 전송을 처리할 수 있는 충분 한 용량이 있습니다.

서버에 대 한 경로를 만들면 데이터는 SAN 스위치를 전송 하는 경우가 가장 많습니다. SAN 스위치는 들어오는 i/o 요청을 처리 하 고 적절 한 포트를 전달 해야 하므로, 스위치는 처리할 수 있는 i/o의 양에 제한이 있지만 제조업체 사양은 해당 제한을 결정 하는 데 필요 합니다. 예를 들어 두 개의 스위치가 있고 각 스위치가 1만 IOPS를 처리할 수 있는 경우 총 처리량은 2만 IOPS가 됩니다. 마찬가지로, 한 스위치가 실패 하는 경우 시스템의 총 처리량은 1만 IOPS가 됩니다. 정상 작업에서 80%의 사용률을 초과 하지 않도록 하는 것이 좋습니다. 8000 i/o를 사용 하 여 대상으로 지정 해야 합니다.

마지막으로 서버에 설치 된 HBA는 처리할 수 있는 i/o의 양에도 제한이 있습니다. 일반적으로 두 번째 HBA는 중복성을 위해 설치 되지만 SAN 스위치를 사용 하는 경우와 마찬가지로 처리할 수 있는 최대 i/o를 계산할 때 *N* 1 hba의 총 처리량은 &ndash; 시스템의 최대 확장성입니다.

### <a name="caching-considerations"></a>캐싱 고려 사항

캐시는 저장소 시스템의 임의 지점에서 전반적인 성능에 큰 영향을 줄 수 있는 구성 요소 중 하나입니다. 캐싱 알고리즘에 대 한 자세한 분석은이 문서의 범위를 벗어났습니다. 그러나 디스크 하위 시스템에 대 한 캐싱에 대 한 몇 가지 기본 문은 다음과 같이 충분 합니다.

- 캐싱은 더 작은 쓰기 작업을 더 큰 i/o 블록으로 버퍼링 하 고 더 적은 수의 블록 크기로 저장소에 대 한 단계를 해제할 수 있으므로 지속적인 순차적 쓰기 i/o i/o를 향상 시킵니다. 그러면 전체 임의 i/o 및 총 순차 i/o가 줄어들고 다른 i/o에 대 한 리소스 가용성이 높아집니다.
- 캐싱은 저장소 하위 시스템의 지속적인 쓰기 i/o 처리량을 향상 시 키 지 않습니다. 스핀 들을 사용 하 여 데이터를 커밋할 때까지 쓰기를 버퍼링 할 수 있습니다. 저장소 하위 시스템에서 사용 가능한 모든 i/o가 오랫동안 포화 상태 이면 캐시가 가득 차게 됩니다. 캐시를 비우기 위해 캐시를 플러시할 수 있는 충분 한 i/o를 제공 하기 위해 버스트 또는 추가 스핀 들 사이에 충분 한 시간을 할당 해야 합니다.

  캐시 크기를 늘리면 더 많은 데이터를 버퍼링 할 수 있습니다. 즉, 더 긴 포화 기간을 수용할 수 있습니다.

  일반적으로 작동 하는 저장소 하위 시스템에서는 데이터를 캐시에만 써야 하므로 운영 체제의 쓰기 성능이 향상 됩니다. 기본 미디어가 i/o를 사용 하 여 포화 상태 이면 캐시가 채워지고 쓰기 성능이 디스크 속도로 돌아옵니다.

- 읽기 i/o를 캐시 하는 경우 캐시는 데이터를 디스크에 순차적으로 저장 하 고 캐시를 미리 읽을 수 있는 경우에 가장 유용 합니다. 다음 섹터에는 다음에 요청 될 데이터가 포함 되어 있다고 가정 합니다.
- 읽기 i/o가 임의의 경우 드라이브 컨트롤러에서 캐싱은 디스크에서 읽을 수 있는 데이터의 양에 대 한 향상 된 기능을 제공 하지 않을 수 있습니다. 운영 체제 또는 응용 프로그램 기반 캐시 크기가 하드웨어 기반 캐시 크기 보다 큰 경우에는 모든 기능이 존재 하지 않습니다.

  Active Directory의 경우 캐시는 RAM의 양에 의해서만 제한 됩니다.

### <a name="ssd-considerations"></a>SSD 고려 사항

Ssd는 스핀 들 기반 하드 디스크와 완전히 다른 동물입니다. 그러나 두 가지 주요 기준은 "처리할 수 있는 IOPS는 몇 개입니까?"입니다. "해당 IOPS의 대기 시간"은 무엇 인가요? 스핀 들 기반 하드 디스크와 비교할 때 Ssd는 더 높은 i/o 볼륨을 처리할 수 있으며 대기 시간이 낮아질 수 있습니다. 일반적으로는이 문서를 작성할 때와 달리, Ssd는 시간당 비용을 비교 하는 데 비용이 많이 듭니다 .이는 i/o 별 비용 면에서 매우 저렴 하며, 저장소 성능 측면에서 상당한 고려 사항이 있습니다.

고려 사항:

- IOPS와 대기 시간은 모두 제조업체 디자인에 매우 주관적 이며 일부 경우에는 스핀 들 기반 기술 보다 쿼리보다 성능이 수행 되는 것으로 관찰 되었습니다. 간단히 말해서 제조업체 사양의 드라이브를 확인 하 고 유효성을 검사 하는 것은 generalities 하지 않는 것이 중요 합니다.
- IOPS 유형은 읽기 또는 쓰기 여부에 따라 숫자가 매우 다를 수 있습니다. 일반적으로 대부분의 읽기 기반 AD DS 서비스는 다른 응용 프로그램 시나리오의 영향을 받지 않습니다.
- "Write endurance" – SSD 셀이 최종적으로 마모 되는 개념입니다. 다양 한 제조업체에서이 챌린지를 서로 다른 fashions 처리 합니다. 데이터베이스 드라이브의 경우에는 최소한의 읽기 i/o 프로필을 사용 하 여 데이터가 너무 volatile이 아닌 중요 한 문제를 최소화할 수 있습니다.

### <a name="summary"></a>요약

저장소에 대해 생각 하는 한 가지 방법은 깊이가 가계 배관입니다. 데이터가 저장 된 미디어의 IOPS가 매우 중요 하다 고 가정 합니다. 이가 clogged (예: 파이프의 루트) 또는 제한 (예: 축소 또는 너무 작음) 된 경우에는 가족의 모든 싱크가 너무 많은 수 (게스트)를 사용 하는 경우에 백업 됩니다. 이는 하나 이상의 시스템이 동일한 기본 미디어를 사용 하 여 SAN/NAS/iSCSI에서 공유 저장소를 활용 하는 공유 환경과 완전히 유사 합니다. 여러 가지 방법을 사용 하 여 다양 한 시나리오를 해결할 수 있습니다.

- 축소 또는의 크기가 부족 드레이닝은 전체 확장 및 수정이 필요 합니다. 이는 인프라 전체에서 공유 저장소를 사용 하 여 시스템을 재배포 하거나 새 하드웨어를 추가 하는 것과 비슷합니다.
- 일반적으로 "clogged" 파이프는 하나 이상의 잘못 된 문제를 식별 하 고 해당 문제를 제거 함을 의미 합니다. 저장소 시나리오에서이는 저장소 또는 시스템 수준 백업, 모든 서버에서 동기화 된 바이러스 검사, 최대 사용 기간 동안 실행 되는 동기화 된 조각 모음 소프트웨어 등이 될 수 있습니다.

모든 배관 설계에서 여러 번의 드레이닝이 주 드레이닝으로 전환 됩니다. 이러한 드레이닝 또는 연결 지점 중 하나를 중지 하는 경우에는 해당 연결 지점 뒤의 항목만 백업 됩니다. 저장소 시나리오에서는 오버 로드 된 스위치 (SAN/NAS/iSCSI 시나리오), 드라이버 호환성 문제 (잘못 된 드라이버/HBA 펌웨어/storport.sys 조합) 또는 백업/바이러스 백신/조각 모음이 있을 수 있습니다. 저장소 "파이프"의 용량이 충분히 큰지 확인 하려면 IOPS 및 i/o 크기를 측정 해야 합니다. 각 조인트에서 적절 한 "파이프 직경"을 보장 하기 위해 함께 추가 합니다.

## <a name="appendix-d---discussion-on-storage-troubleshooting---environments-where-providing-at-least-as-much-ram-as-the-database-size-is-not-a-viable-option"></a>부록 D-저장소 문제 해결에 대 한 논의-데이터베이스 크기와 최소한의 RAM을 제공 하는 것은 실행 가능한 옵션이 아닙니다.

저장소 기술의 변경 사항을 수용할 수 있도록 이러한 권장 사항이 있는 이유를 이해 하는 것이 좋습니다. 이러한 권장 사항은 두 가지 이유로 존재 합니다. 첫 번째는 IO의 격리로, 운영 체제 스핀 들의 성능 문제 (즉, 페이징)는 데이터베이스 및 i/o 프로필의 성능에 영향을 주지 않습니다. 두 번째는 AD DS (및 대부분의 데이터베이스)에 대 한 로그 파일은 기본적으로 순차적으로 수행 되며, 스핀 들 기반 하드 드라이브 및 캐시는 운영 체제의 보다 임의 i/o 패턴과 AD DS 데이터베이스 드라이브의 거의 임의 i/o 패턴에 비해 순차적 i/o와 함께 사용 될 경우 상당한 성능상의 이점을 제공 합니다. 순차 i/o를 별도의 물리적 드라이브로 격리 하면 처리량을 늘릴 수 있습니다. 오늘날의 저장소 옵션에서 제공 하는 과제는 이러한 권장 사항에 대 한 근본적인 가정이 더 이상 true가 아니라는 것입니다. ISCSI, SAN, NAS, 가상 디스크 이미지 파일 등의 가상화 된 많은 저장소 시나리오에서 기본 저장소 미디어는 여러 호스트에서 공유 되므로 "IO의 격리" 및 "순차적 i/o 최적화" 측면을 완전히 부정 합니다. 실제로 이러한 시나리오에서는 공유 미디어에 액세스 하는 다른 호스트에서 도메인 컨트롤러에 대 한 응답성을 저하 시킬 수 있다는 점에서 더 복잡 한 계층을 추가 합니다.

저장소 성능 계획에는 콜드 캐시 상태, 준비 cache 상태, 백업/복원 이라는 세 가지 범주를 고려해 야 합니다. 콜드 캐시 상태는 도메인 컨트롤러를 처음 다시 부팅 하거나 Active Directory 서비스가 다시 시작 되 고 RAM에 Active Directory 데이터가 없는 경우와 같은 시나리오에서 발생 합니다. 웜 캐시 상태는 도메인 컨트롤러가 안정 된 상태이 고 데이터베이스가 캐시 되는 위치입니다. 이러한 정보는 매우 다양 한 성능 프로필을 구동 하 고, 전체 데이터베이스를 캐시 하는 데 충분 한 RAM을 확보 하는 것이 중요 하므로 캐시가 콜드 인 경우 성능이 향상 되지 않습니다. 이러한 두 시나리오에 대 한 성능 디자인을 고려할 수 있습니다 .이는 콜드 캐시가 "스 프린트"이 고 웜 캐시를 사용 하 여 서버를 실행 하는 것은 "marathon"입니다.

콜드 캐시와 웜 캐시 시나리오 둘 다에 대 한 질문은 저장소가 디스크에서 메모리로 데이터를 이동할 수 있는 속도입니다. 캐시는 시간이 지남에 따라 더 많은 쿼리가 데이터를 재사용 하 고, 캐시 적중률이 증가 하 고, 디스크로 이동 해야 하는 빈도가 감소 함에 따라 성능이 향상 되는 시나리오입니다. 따라서 디스크로 이동 하는 경우 성능에 부정적인 영향을 미칠 수 있습니다. 성능이 저하 되는 것은 캐시의 준비를 대기 하는 동안에만 일시적 이며 시스템에 종속 된 최대 크기로 확장할 수 있습니다. 이 대화는 디스크에서 데이터를 얻을 수 있는 속도를 단순화 하 고, 기본 저장소에서 사용할 수 있는 IOPS에 대 한 주관적 인 Active Directory에 사용할 수 있는 IOPS를 간단히 측정 하는 데 사용할 수 있습니다. 계획 관점에서, 캐시 및 백업/복원 시나리오를 준비 하는 작업은 예외적으로 발생 하 고, 일반적으로 몇 시간 내에 발생 하며, DC를 로드 하는 데 주관적인 것 이기 때문에 이러한 활동이 사용량이 많지 않은 시간에 대해 예약 되는 경우를 제외 하 고 일반 권장 사항이 존재 하지 않습니다.

대부분의 시나리오에서 AD DS은 일반적으로 읽기 IO 이며 일반적으로 90% 읽기/10% 쓰기의 비율입니다. 읽기 i/o는 사용자 환경에 병목 현상이 발생 하 고 쓰기 IO를 사용 하면 쓰기 성능이 저하 되는 경향이 있습니다. Ntds.dit의 경우에는 dit가 거의 무작위로 수행 됩니다. 캐시는 IO 읽기에 최소한의 이점을 제공 하 여 읽기 i/o 프로필에 대 한 저장소를 올바르게 구성 하는 것이 매우 중요 하다는 경향이 있습니다.

정상적인 운영 조건의 경우 저장소 계획 목표는 AD DS 요청이 디스크에서 반환 될 때 까지의 대기 시간을 최소화 하는 것입니다. 이는 기본적으로 미해결 및 보류 중인 i/o 수가 디스크에 대 한 경로 수에 해당 하는 값 보다 작거나 같은지 여부를 의미 합니다. 이를 측정 하는 방법에는 여러 가지가 있습니다. 성능 모니터링 시나리오에서 일반적인 권장 사항은 논리 디스크 ( *\<NTDS Database Drive\>* ) \Avg Disk sec/Read를 20 밀리초 미만으로 하는 것입니다. 원하는 운영 임계값은 저장소의 유형에 따라 2-6 밀리초 (002 to .006 second) 범위에서 가능한 한 저장소 속도와 같이 훨씬 더 작아야 합니다.

예:

![저장소 대기 시간 차트](media/capacity-planning-considerations-storage-latency.png)

차트 분석:

- **왼쪽의 녹색 타원형 –** 대기 시간은 10 밀리초와 동일 하 게 유지 됩니다. 부하가 800 IOPS에서 2400 IOPS로 늘어납니다. 이는 기본 저장소에서 i/o 요청을 처리할 수 있는 속도에 대 한 절대 바닥입니다. 이는 저장소 솔루션의 세부 사항에 따라 달라 집니다.
- **오른쪽의 포도주 oval –** 대기 시간이 계속 증가 하는 동안에는 녹색 원의 끝부터 데이터 컬렉션 끝 까지의 처리량이 균일 하 게 유지 됩니다. 이는 요청 볼륨이 기본 저장소의 물리적인 제한을 초과 하는 경우 요청이 저장소 하위 시스템으로 전송 되기를 기다리는 큐에 더 오래 걸리는 것을 보여 줍니다.

이 정보를 적용 하는 중:

- **큰 그룹의 멤버 자격을 쿼리 하는 사용자에 게 미치는 영향 –** 디스크에서 1mb의 데이터를 읽어야 한다고 가정 하 고 i/o 양과 소요 시간이 다음과 같이 계산 될 수 있다고 가정 합니다.
  - Active Directory 데이터베이스 페이지의 크기는 8kb입니다.
  - 디스크에서 최소 128 페이지를 읽어야 합니다.
  - 아무것도 캐시 되지 않는다고 가정할 경우 (10 밀리초),이는 클라이언트에 반환 하기 위해 디스크에서 데이터를 로드 하는 데 최소 1.28 초 정도 걸립니다. 최대값 out부터 저장소에 대 한 처리량이 길고 권장 되는 최대 크기 이기도 한 20 밀리초의 경우, 최종 사용자에 게 반환 하기 위해 디스크에서 데이터를 가져오는 데 2.5 초가 소요 됩니다.
- **캐시가 준비 될 속도는 무엇 인가요?** 이 저장소 예제의 처리량을 최대화 하기 위해 클라이언트 로드를 결정 하는 경우 캐시는 IO 당 2400 IOPS 8kb 속도로 준비 됩니다 &times; . 또는 초당 약 20 m b/초, 53 초 마다 RAM에 1gb의 데이터베이스를 로드 합니다.

> [!NOTE]
> 시스템을 백업 하거나 AD DS 가비지 수집을 실행 하는 경우와 같이 구성 요소에서 디스크를 적극적으로 읽거나 쓸 때 대기 시간을 확인 하는 것이 일반적입니다. 이러한 정기 이벤트를 수용 하기 위해 계산 위에 추가 헤드를 제공 해야 합니다. 정상적인 기능에 영향을 주지 않고 이러한 시나리오를 수용 하기에 충분 한 처리량을 제공 하는 목표입니다.

볼 수 있듯이 캐시의 준비 속도는 저장소 디자인에 따라 결정 됩니다. 캐시에는 들어오는 클라이언트 요청이 기본 저장소에서 제공할 수 있는 속도까지 제공 됩니다. 가장 많은 시간 동안 캐시를 "미리 준비" 하는 스크립트를 실행 하면 실제 클라이언트 요청에 따라 부하를 분산할 수 있습니다. 이렇게 하면 클라이언트에서 가장 먼저 필요한 데이터를 배달 하는 데 부정적인 영향을 줄 수 있습니다 .이는 캐시에 대 한 인위적인 시도로 인해 DC에 연결 하는 클라이언트와 관련이 없는 데이터를 로드 하기 때문에 디스크 리소스에 대 한 경쟁을 생성 합니다.
